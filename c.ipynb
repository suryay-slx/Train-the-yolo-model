{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d55284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8 â€” Dataset Check â†’ (optional) Auto-fix â†’ Visualize â†’ Train â†’ Validate (STOP HERE)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== EDIT THESE ======\n",
    "DATASET_DIR = Path(r\"/home/surya/Downloads/soccer players.v1i.yolov8\")  # your extracted dataset folder\n",
    "PROJECT_NAME  = \"soccer_players_clean\"\n",
    "IMG_SIZE      = 640       # try 960/1280 for small objects (applies to both width and height)\n",
    "EPOCHS        = 80        # raise if dataset is small (100â€“150)\n",
    "BATCH         = 4\n",
    "MODEL_WEIGHTS = \"yolov8n.pt\"  # try yolov8s.pt later\n",
    "DO_AUTOFIX    = True      # set False to only report issues\n",
    "N_VIS_SAMPLES = 24        # annotated preview images per split\n",
    "# ========================\n",
    "\n",
    "ROOT       = Path.cwd()\n",
    "WORK_DIR   = ROOT / \"datasets\" / PROJECT_NAME\n",
    "REPORT_DIR = ROOT / \"dataset_report\"\n",
    "\n",
    "WORK_DIR, REPORT_DIR\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIRS = {\n",
    "    \"train\": DATASET_DIR / \"images\" / \"train\",\n",
    "    \"val\":   DATASET_DIR / \"images\" / \"val\",\n",
    "    \"test\":  DATASET_DIR / \"images\" / \"test\",\n",
    "}\n",
    "LABEL_DIRS = {\n",
    "    \"train\": DATASET_DIR / \"labels\" / \"train\",\n",
    "    \"val\":   DATASET_DIR / \"labels\" / \"val\",\n",
    "    \"test\":  DATASET_DIR / \"labels\" / \"test\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8800dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in ./.venv/lib/python3.12/site-packages (8.3.185)\n",
      "Requirement already satisfied: supervision in ./.venv/lib/python3.12/site-packages (0.26.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.5)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: opencv-python-headless in ./.venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy>=1.23.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./.venv/lib/python3.12/site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./.venv/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.12/site-packages (from ultralytics) (1.16.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./.venv/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./.venv/lib/python3.12/site-packages (from ultralytics) (2.0.16)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in ./.venv/lib/python3.12/site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
      "Platform: Linux-6.14.0-28-generic-x86_64-with-glibc2.39\n",
      "Torch: 2.8.0+cu128\n",
      "CUDA available: False\n",
      "nvidia-smi not available or failed: [Errno 2] No such file or directory: 'nvidia-smi'\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics supervision matplotlib pandas seaborn opencv-python-headless\n",
    "import sys, platform, torch, subprocess, shlex\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Compute capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"Total VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1e9,2))\n",
    "try:\n",
    "    print(subprocess.check_output(shlex.split(\"nvidia-smi\"), text=True))\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available or failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec48587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing mirrored dataset: /home/surya/c/datasets/soccer players.v1i.yolov8\n",
      "DATA_ROOT: /home/surya/c/datasets/soccer players.v1i.yolov8\n",
      "data.yaml: /home/surya/c/datasets/soccer players.v1i.yolov8/data.yaml\n",
      "train images: /home/surya/c/datasets/soccer players.v1i.yolov8/../train/images exists: False\n",
      "val images:   /home/surya/c/datasets/soccer players.v1i.yolov8/../valid/images exists: False\n",
      "test images:  /home/surya/c/datasets/soccer players.v1i.yolov8/../test/images exists: False\n"
     ]
    }
   ],
   "source": [
    "import shutil, yaml, os\n",
    "from pathlib import Path\n",
    "\n",
    "def mirror_dataset(src: Path, dst_root: Path) -> Path:\n",
    "    assert src.is_dir(), f\"Dataset folder not found: {src}\"\n",
    "    dst = dst_root / src.name\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not dst.exists() or not any(dst.iterdir()):\n",
    "        print(f\"Copying {src} -> {dst}\")\n",
    "        if dst.exists():\n",
    "            shutil.rmtree(dst)\n",
    "        shutil.copytree(src, dst)\n",
    "    else:\n",
    "        print(f\"Using existing mirrored dataset: {dst}\")\n",
    "    return dst\n",
    "\n",
    "def find_data_yaml(base: Path):\n",
    "    cand = base / \"data.yaml\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    for sd in [p for p in base.iterdir() if p.is_dir()]:\n",
    "        c = sd / \"data.yaml\"\n",
    "        if c.exists():\n",
    "            return c\n",
    "    allc = list(base.rglob(\"data.yaml\"))\n",
    "    if allc:\n",
    "        allc.sort(key=lambda p: len(p.parts))\n",
    "        return allc[0]\n",
    "    return None\n",
    "\n",
    "DATA_ROOT = mirror_dataset(DATASET_DIR, WORK_DIR.parent)\n",
    "DATA_YAML = find_data_yaml(DATA_ROOT)\n",
    "assert DATA_YAML and DATA_YAML.exists(), f\"data.yaml not found under {DATA_ROOT}\"\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"data.yaml:\", DATA_YAML)\n",
    "\n",
    "with open(DATA_YAML, \"r\") as f:\n",
    "    spec = yaml.safe_load(f)\n",
    "\n",
    "base = Path(spec.get(\"path\", DATA_YAML.parent))\n",
    "\n",
    "def resolve_dir(key, default):\n",
    "    p = spec.get(key, default)\n",
    "    p = Path(p)\n",
    "    if not p.is_absolute():\n",
    "        p = base / p\n",
    "    return p\n",
    "\n",
    "val_key = \"val\" if \"val\" in spec else (\"valid\" if \"valid\" in spec else \"val\")\n",
    "train_images = resolve_dir(\"train\", \"train/images\")\n",
    "val_images   = resolve_dir(val_key, f\"{val_key}/images\")\n",
    "test_images  = resolve_dir(\"test\",  \"test/images\")\n",
    "\n",
    "print(\"train images:\", train_images, \"exists:\", train_images.exists())\n",
    "print(\"val images:  \", val_images,   \"exists:\", val_images.exists())\n",
    "print(\"test images: \", test_images,  \"exists:\", test_images.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c15239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0', '1', '2']  (count: 3 )\n",
      "Issues found: 0  -> /home/surya/c/dataset_report/dataset_issues.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_classes(spec):\n",
    "    names = spec.get(\"names\", [])\n",
    "    if isinstance(names, dict):\n",
    "        names = [names[i] for i in sorted(map(int, names.keys()))]\n",
    "    return names, len(names)\n",
    "\n",
    "CLASS_NAMES, NUM_CLASSES = load_classes(spec)\n",
    "print(\"Classes:\", CLASS_NAMES, \" (count:\", NUM_CLASSES, \")\")\n",
    "\n",
    "def image_label_pairs(split_images: Path):\n",
    "    pairs = []\n",
    "    for img in split_images.rglob(\"*\"):\n",
    "        if img.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}:\n",
    "            rel = img.relative_to(split_images)\n",
    "            lbl = split_images.parent / \"labels\" / rel.with_suffix(\".txt\")\n",
    "            pairs.append((img, lbl))\n",
    "    return pairs\n",
    "\n",
    "def parse_label_file(lbl_path: Path):\n",
    "    lines = []\n",
    "    if not lbl_path.exists():\n",
    "        return lines\n",
    "    for i, line in enumerate(lbl_path.read_text().splitlines(), 1):\n",
    "        line=line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) != 5:\n",
    "            lines.append((\"bad_format\", i, line)); continue\n",
    "        try:\n",
    "            cid = int(float(parts[0])); x,y,w,h = map(float, parts[1:])\n",
    "        except Exception:\n",
    "            lines.append((\"parse_error\", i, line)); continue\n",
    "        lines.append((\"ok\", i, (cid,x,y,w,h)))\n",
    "    return lines\n",
    "\n",
    "def validate_split(split_name, split_images):\n",
    "    pairs = image_label_pairs(split_images)\n",
    "    issues = []\n",
    "    class_counts = Counter()\n",
    "    tiny_counts = Counter()\n",
    "    for img, lbl in pairs:\n",
    "        if not lbl.exists():\n",
    "            issues.append((split_name, \"missing_label\", str(img), \"\")); continue\n",
    "        parsed = parse_label_file(lbl)\n",
    "        if not parsed:\n",
    "            continue\n",
    "        for status, ln, payload in parsed:\n",
    "            if status != \"ok\":\n",
    "                issues.append((split_name, status, str(lbl), f\"line {ln}: {payload}\")); continue\n",
    "            cid,x,y,w,h = payload\n",
    "            if cid < 0 or cid >= NUM_CLASSES:\n",
    "                issues.append((split_name, \"bad_class_id\", str(lbl), f\"line {ln}: {cid} not in [0,{NUM_CLASSES-1}]\")); continue\n",
    "            if not (0.0 <= x <= 1.0 and 0.0 <= y <= 1.0 and 0.0 < w <= 1.0 and 0.0 < h <= 1.0):\n",
    "                issues.append((split_name, \"out_of_range\", str(lbl), f\"line {ln}: {x},{y},{w},{h}\")); continue\n",
    "            area = w*h\n",
    "            if area < 0.005:   # <0.5% of image area (tiny)\n",
    "                tiny_counts[cid]+=1\n",
    "                issues.append((split_name, \"tiny_box_warn\", str(lbl), f\"line {ln}: area={area:.5f}\"))\n",
    "            class_counts[cid]+=1\n",
    "\n",
    "    # labels without images\n",
    "    labels_path = split_images.parent / \"labels\"\n",
    "    for lbl in labels_path.rglob(\"*.txt\"):\n",
    "        rel = lbl.relative_to(labels_path)\n",
    "        img = split_images / rel.with_suffix(\".jpg\")\n",
    "        if not img.exists():\n",
    "            found=False\n",
    "            for ext in [\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".JPG\",\".PNG\"]:\n",
    "                if (split_images / rel.with_suffix(ext)).exists():\n",
    "                    found=True; break\n",
    "            if not found:\n",
    "                issues.append((split_name, \"missing_image_for_label\", str(lbl), \"\"))\n",
    "\n",
    "    return pairs, issues, class_counts, tiny_counts\n",
    "\n",
    "all_issues = []\n",
    "split_stats = {}\n",
    "for split_name, folder in [(\"train\", train_images), (val_key, val_images), (\"test\", test_images)]:\n",
    "    if folder.exists():\n",
    "        pairs, issues, class_counts, tiny_counts = validate_split(split_name, folder)\n",
    "        split_stats[split_name] = {\"images\": len(pairs), \"class_counts\": dict(class_counts), \"tiny_counts\": dict(tiny_counts)}\n",
    "        all_issues.extend(issues)\n",
    "\n",
    "issues_df = pd.DataFrame(all_issues, columns=[\"split\",\"issue\",\"path\",\"detail\"]) if all_issues else pd.DataFrame(columns=[\"split\",\"issue\",\"path\",\"detail\"])\n",
    "issues_csv = REPORT_DIR / \"dataset_issues.csv\"\n",
    "issues_df.to_csv(issues_csv, index=False)\n",
    "print(f\"Issues found: {len(issues_df)}  -> {issues_csv}\")\n",
    "split_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed75ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved class distribution -> /home/surya/c/dataset_report/class_distribution.csv\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def summarize_distribution(split_stats, class_names):\n",
    "    rows = []\n",
    "    for split, st in split_stats.items():\n",
    "        row = {\"split\": split, \"images\": st[\"images\"]}\n",
    "        for i, cname in enumerate(class_names):\n",
    "            row[f\"class_{i}:{cname}\"] = st[\"class_counts\"].get(i, 0)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "dist_df = summarize_distribution(split_stats, CLASS_NAMES)\n",
    "dist_csv = REPORT_DIR / \"class_distribution.csv\"\n",
    "dist_df.to_csv(dist_csv, index=False)\n",
    "print(\"Saved class distribution ->\", dist_csv)\n",
    "display(dist_df) if 'display' in globals() else print(dist_df)\n",
    "\n",
    "def image_label_pairs(split_images: Path):\n",
    "    pairs = []\n",
    "    for img in split_images.rglob(\"*\"):\n",
    "        if img.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}:\n",
    "            rel = img.relative_to(split_images)\n",
    "            lbl = split_images.parent / \"labels\" / rel.with_suffix(\".txt\")\n",
    "            pairs.append((img, lbl))\n",
    "    return pairs\n",
    "\n",
    "def draw_yolo_boxes(img_path: Path, lbl_path: Path, names):\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        return None\n",
    "    h, w = img.shape[:2]\n",
    "    colors = {i: ((37*i)%255, (17*i)%255, (97*i)%255) for i in range(len(names))}\n",
    "    if lbl_path.exists():\n",
    "        for line in lbl_path.read_text().splitlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5: \n",
    "                continue\n",
    "            try:\n",
    "                cid = int(float(parts[0])); xc, yc, bw, bh = map(float, parts[1:])\n",
    "            except:\n",
    "                continue\n",
    "            x1 = int((xc - bw/2) * w); y1 = int((yc - bh/2) * h)\n",
    "            ww = int(bw * w); hh = int(bh * h)\n",
    "            x1 = max(0, x1); y1 = max(0, y1); ww = max(1, ww); hh = max(1, hh)\n",
    "            cv2.rectangle(img, (x1,y1), (x1+ww,y1+hh), colors.get(cid,(0,255,0)), 2)\n",
    "            cv2.putText(img, names[cid] if cid<len(names) else str(cid), (x1,y1-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors.get(cid,(0,255,0)), 2)\n",
    "    return img\n",
    "\n",
    "def save_previews(split_name, split_images, n=24):\n",
    "    out_dir = REPORT_DIR / \"previews\" / split_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pairs = image_label_pairs(split_images)\n",
    "    random.seed(0); random.shuffle(pairs)\n",
    "    saved = 0\n",
    "    for img, lbl in pairs:\n",
    "        vis = draw_yolo_boxes(img, lbl, CLASS_NAMES)\n",
    "        if vis is None: \n",
    "            continue\n",
    "        out = out_dir / f\"{img.stem}_preview.jpg\"\n",
    "        cv2.imwrite(str(out), vis)\n",
    "        saved += 1\n",
    "        if saved >= n:\n",
    "            break\n",
    "    print(f\"Saved {saved} previews -> {out_dir}\")\n",
    "\n",
    "for split_name, folder in [(\"train\", train_images), (val_key, val_images), (\"test\", test_images)]:\n",
    "    if folder.exists():\n",
    "        save_previews(split_name, folder, N_VIS_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bb6ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-fix issues: 0  -> /home/surya/c/dataset_report/dataset_issues_postfix.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def autofix_labels(split_images, drop_empty=False):\n",
    "    fixed, dropped = 0, 0\n",
    "    labels_path = split_images.parent / \"labels\"\n",
    "    for lbl in labels_path.rglob(\"*.txt\"):\n",
    "        lines_in = lbl.read_text().splitlines() if lbl.exists() else []\n",
    "        new_lines, changed = [], False\n",
    "        for line in lines_in:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                dropped += 1; changed=True; continue\n",
    "            try:\n",
    "                cid = int(float(parts[0])); xc, yc, bw, bh = map(float, parts[1:])\n",
    "            except:\n",
    "                dropped += 1; changed=True; continue\n",
    "            if cid < 0 or cid >= NUM_CLASSES:\n",
    "                dropped += 1; changed=True; continue\n",
    "            # clip to [0,1], avoid zero size\n",
    "            orig = (xc,yc,bw,bh)\n",
    "            xc = min(max(xc, 0.0), 1.0)\n",
    "            yc = min(max(yc, 0.0), 1.0)\n",
    "            bw = min(max(bw, 1e-6), 1.0)\n",
    "            bh = min(max(bh, 1e-6), 1.0)\n",
    "            if bw <= 0 or bh <= 0:\n",
    "                dropped += 1; changed=True; continue\n",
    "            if orig != (xc,yc,bw,bh):\n",
    "                changed = True\n",
    "            new_lines.append(f\"{cid} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\")\n",
    "        if drop_empty and not new_lines:\n",
    "            lbl.unlink(missing_ok=True); changed=True\n",
    "        if changed:\n",
    "            lbl.write_text(\"\\n\".join(new_lines)); fixed += 1\n",
    "    return fixed, dropped\n",
    "\n",
    "if DO_AUTOFIX:\n",
    "    for split_name, folder in [(\"train\", train_images), (val_key, val_images), (\"test\", test_images)]:\n",
    "        if folder.exists():\n",
    "            fixed, dropped = autofix_labels(folder, drop_empty=False)\n",
    "            print(f\"[{split_name}] files fixed: {fixed}, lines dropped: {dropped}\")\n",
    "\n",
    "# quick re-validate\n",
    "all_issues2 = []\n",
    "split_stats2 = {}\n",
    "for split_name, folder in [(\"train\", train_images), (val_key, val_images), (\"test\", test_images)]:\n",
    "    if folder.exists():\n",
    "        pairs, issues, class_counts, tiny_counts = validate_split(split_name, folder)\n",
    "        split_stats2[split_name] = {\"images\": len(pairs), \"class_counts\": dict(class_counts), \"tiny_counts\": dict(tiny_counts)}\n",
    "        all_issues2.extend(issues)\n",
    "\n",
    "issues2_df = pd.DataFrame(all_issues2, columns=[\"split\",\"issue\",\"path\",\"detail\"]) if all_issues2 else pd.DataFrame(columns=[\"split\",\"issue\",\"path\",\"detail\"])\n",
    "issues2_csv = REPORT_DIR / \"dataset_issues_postfix.csv\"\n",
    "issues2_df.to_csv(issues2_csv, index=False)\n",
    "print(f\"Post-fix issues: {len(issues2_df)}  -> {issues2_csv}\")\n",
    "split_stats2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37deac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n",
      "Ultralytics 8.3.185 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CPU (11th Gen Intel Core(TM) i5-11300H 3.10GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/surya/c/datasets/soccer players.v1i.yolov8/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=80, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=soccer_players_clean-exp2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/soccer_players_clean-exp2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 86.1Â±58.2 MB/s, size: 29.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/surya/c/datasets/soccer players.v1i.yolov8/train/labels.cache... 114 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 145.7Â±87.5 MB/s, size: 32.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/surya/c/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/surya/c/datasets/soccer players.v1i.yolov8/valid/labels.cache... 33 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<?, ?it/s]\n",
      "/home/surya/c/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/soccer_players_clean-exp2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/soccer_players_clean-exp2\u001b[0m\n",
      "Starting training for 80 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/80         0G      1.785      2.919      1.079         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:27<00:00,  3.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:34<00:00,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407     0.0324      0.415      0.219      0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/80         0G      1.556      1.472      1.013         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [08:15<00:00, 17.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:34<00:00,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.964       0.22      0.297      0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/80         0G      1.547       1.34      1.015         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:05<00:00, 10.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.956      0.306      0.328      0.174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/80         0G      1.484      1.193      1.005         61        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.965      0.318      0.337      0.195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/80         0G      1.461      1.207       1.03         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:00<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.651      0.363      0.341      0.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/80         0G      1.442       1.17      1.011         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.614      0.368      0.368      0.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/80         0G      1.412      1.143     0.9959         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.611      0.392      0.383      0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/80         0G       1.38      1.049     0.9915         52        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.967      0.313      0.424      0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/80         0G      1.397      1.058      1.012         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.968      0.309      0.435      0.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/80         0G      1.416      1.018     0.9991         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.501      0.539      0.511      0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/80         0G      1.376      1.003     0.9916         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.51       0.55      0.556      0.297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/80         0G      1.372      1.008      1.004         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.519      0.499      0.538      0.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/80         0G      1.344     0.9612     0.9865         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.537      0.534      0.543      0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/80         0G      1.316     0.9277     0.9884         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:00<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.858       0.53      0.549      0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/80         0G      1.328       0.93     0.9765         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.892      0.512      0.555      0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/80         0G      1.348      0.912     0.9616         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.902      0.505      0.547      0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/80         0G      1.326     0.8805     0.9743         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.89      0.509      0.566      0.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/80         0G      1.337     0.8859     0.9575         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:00<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.906      0.501      0.553      0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/80         0G      1.264     0.8695     0.9635         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.726      0.613      0.697        0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/80         0G      1.262     0.8342     0.9643         73        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:00<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.761      0.691       0.75      0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/80         0G      1.263     0.8213      0.955         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.798      0.734      0.805      0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/80         0G      1.243     0.7885     0.9524         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.829      0.784      0.868      0.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/80         0G      1.251     0.7611     0.9579         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.957      0.829      0.888      0.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/80         0G      1.251     0.7511     0.9456         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.947      0.831      0.883      0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/80         0G      1.234     0.7091     0.9439         60        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.92      0.838      0.891      0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/80         0G      1.232     0.7284     0.9455         84        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.922      0.844      0.888      0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/80         0G      1.266     0.7451     0.9635         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.934      0.846      0.883      0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/80         0G      1.238     0.7219      0.941         81        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.963      0.841      0.894      0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/80         0G      1.216     0.7199     0.9622         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:59<00:00,  2.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.956      0.835      0.903      0.498\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      30/80         0G      1.264     0.7219      0.937         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:54<00:00,  1.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.971      0.819      0.908      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/80         0G      1.232     0.7052     0.9495         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.957      0.828      0.905      0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/80         0G      1.197     0.6774     0.9514         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.944      0.835       0.89      0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/80         0G      1.194     0.6598     0.9246         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.954      0.839      0.902      0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/80         0G      1.211     0.6607     0.9362         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.948       0.84      0.884      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/80         0G      1.183     0.6681     0.9442         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.92      0.852      0.907      0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/80         0G      1.157     0.6551     0.9371         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.979      0.842      0.905      0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/80         0G      1.183      0.643     0.9421         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:59<00:00,  2.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.959      0.848      0.905      0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/80         0G      1.141     0.6422     0.9151         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.959      0.846      0.905      0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/80         0G      1.143     0.6223     0.9219         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.991      0.834      0.904       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/80         0G      1.151      0.634     0.9416         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:03<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.907      0.859       0.91      0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/80         0G      1.153     0.6314     0.9224         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.919      0.847      0.905      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/80         0G      1.195     0.6307     0.9352         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:47<00:00,  1.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.969       0.84      0.882      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      43/80         0G      1.195     0.6456     0.9403         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:48<00:00,  1.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.959      0.843      0.901      0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/80         0G      1.153     0.6251     0.9228         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.957      0.844      0.903       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/80         0G      1.172     0.6283      0.949         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.924       0.87      0.901      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/80         0G      1.167     0.6257      0.947         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.924      0.865      0.901      0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/80         0G      1.135     0.5979     0.9286         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.934      0.863      0.904      0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/80         0G      1.156     0.6084     0.9232         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.97       0.85      0.902      0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/80         0G      1.165     0.6234     0.9359         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.963      0.847      0.901      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/80         0G      1.148     0.5948     0.9155         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.961      0.842       0.91       0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      51/80         0G      1.165     0.6122     0.9402         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:03<00:00,  2.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.94      0.868      0.896      0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      52/80         0G      1.146     0.5981     0.9159         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.94      0.862      0.908      0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      53/80         0G      1.124     0.5928     0.9307         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.955      0.853      0.909      0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      54/80         0G      1.107      0.593      0.919         60        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.955      0.853      0.901       0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      55/80         0G      1.098     0.5837     0.9166         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.923      0.856      0.897      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      56/80         0G       1.11     0.5906     0.9181         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.935      0.867      0.894      0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      57/80         0G      1.122      0.586     0.9151         74        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.955       0.87      0.898      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      58/80         0G      1.119     0.5867      0.922         52        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.949      0.869      0.898      0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      59/80         0G      1.119     0.5792     0.9117         62        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.94      0.865      0.903      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      60/80         0G      1.112     0.5743     0.9218         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.935      0.871      0.926      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      61/80         0G      1.159     0.5969     0.9258         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.946       0.87      0.907      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      62/80         0G      1.083     0.5796     0.9232         81        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.938      0.869      0.908      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      63/80         0G      1.116     0.5845     0.9226         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.943      0.868      0.917      0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      64/80         0G      1.109     0.5763     0.9179         77        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.965      0.869      0.918      0.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      65/80         0G      1.108      0.566     0.9182         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.961      0.869      0.908      0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      66/80         0G      1.097     0.5659     0.9072         52        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.964      0.869      0.907      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      67/80         0G      1.089     0.5628     0.9078         57        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.971      0.868      0.915      0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      68/80         0G      1.055     0.5443     0.9104         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:03<00:00,  2.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.971       0.87      0.915      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      69/80         0G      1.071     0.5634     0.9161         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.965      0.872      0.914      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      70/80         0G      1.086     0.5639     0.9055         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.964      0.872      0.913      0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surya/c/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      71/80         0G       1.07     0.5863      0.923         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407       0.97      0.858      0.909       0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      72/80         0G      1.074     0.5797     0.9113         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.952      0.857      0.901      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      73/80         0G      1.065     0.5716     0.9167         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.949      0.858      0.902      0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      74/80         0G      1.058     0.5663     0.9142         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.931      0.869      0.905      0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      75/80         0G      1.043     0.5676     0.9127         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.949      0.855      0.906      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      76/80         0G      1.038     0.5541     0.9137         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:54<00:00,  1.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.938      0.866      0.906      0.555\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      77/80         0G      1.041     0.5518     0.9058         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:56<00:00,  1.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.941      0.864      0.909      0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      78/80         0G      1.028     0.5497     0.9036         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:02<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.947      0.867       0.91      0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      79/80         0G      1.044     0.5583     0.8998         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.947      0.867      0.911      0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      80/80         0G      1.054     0.5633     0.9038         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:01<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.949      0.867      0.911      0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "80 epochs completed in 1.662 hours.\n",
      "Optimizer stripped from runs/train/soccer_players_clean-exp2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/train/soccer_players_clean-exp2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/train/soccer_players_clean-exp2/weights/best.pt...\n",
      "Ultralytics 8.3.185 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CPU (11th Gen Intel Core(TM) i5-11300H 3.10GHz)\n",
      "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33        407      0.965      0.869      0.918      0.569\n",
      "                     0         33        359       0.99      0.978      0.991      0.649\n",
      "                     1         20         20      0.962       0.95      0.976      0.714\n",
      "                     2         28         28      0.942      0.679      0.787      0.345\n",
      "Speed: 3.1ms preprocess, 92.9ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/soccer_players_clean-exp2\u001b[0m\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip -q install ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"GPU available:\", use_gpu)\n",
    "\n",
    "model = YOLO(MODEL_WEIGHTS)\n",
    "\n",
    "# (Optional) 1-epoch smoke test on CPU:\n",
    "# model.train(data=str(DATA_YAML), imgsz=320, batch=2, epochs=1, device=\"cpu\", workers=0, name=f\"{PROJECT_NAME}-smoke_cpu\")\n",
    "\n",
    "# Proper training run\n",
    "res = model.train(\n",
    "    data=str(DATA_YAML),\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH,\n",
    "    epochs=EPOCHS,\n",
    "    device=0 if use_gpu else \"cpu\",\n",
    "    workers=0,     # notebook-safe\n",
    "    amp=True,      # mixed precision\n",
    "    cache=False,   # enable only if you have lots of RAM\n",
    "    project=\"runs/train\",\n",
    "    name=f\"{PROJECT_NAME}-exp\",\n",
    ")\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.185 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CPU (11th Gen Intel Core(TM) i5-11300H 3.10GHz)\n",
      "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/train/soccer_players_clean-exp/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (17.6 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.59', 'onnxruntime'] not found, attempting AutoUpdate...\n",
      "Collecting onnx<1.18.0,>=1.12.0\n",
      "  Downloading onnx-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxslim>=0.1.59\n",
      "  Downloading onnxslim-0.1.65-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.12/site-packages (from onnx<1.18.0,>=1.12.0) (2.2.6)\n",
      "Collecting protobuf>=3.20.2 (from onnx<1.18.0,>=1.12.0)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from onnxslim>=0.1.59) (1.14.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from onnxslim>=0.1.59) (25.0)\n",
      "Collecting colorama (from onnxslim>=0.1.59)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->onnxslim>=0.1.59) (1.3.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading onnx-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxslim-0.1.65-py3-none-any.whl (164 kB)\n",
      "Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: flatbuffers, protobuf, humanfriendly, colorama, onnx, coloredlogs, onnxslim, onnxruntime\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8/8\u001b[0m [onnxruntime]\u001b[0m [onnxruntime]\n",
      "\u001b[1A\u001b[2KSuccessfully installed colorama-0.4.6 coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.22.1 onnxslim-0.1.65 protobuf-6.32.0\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 14.8s\n",
      "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 17.3s, saved as 'runs/train/soccer_players_clean-exp/weights/best.onnx' (11.7 MB)\n",
      "\n",
      "Export complete (18.7s)\n",
      "Results saved to \u001b[1m/home/surya/c/runs/train/soccer_players_clean-exp/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs/train/soccer_players_clean-exp/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs/train/soccer_players_clean-exp/weights/best.onnx imgsz=640 data=/home/surya/c/datasets/soccer players.v1i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Exported ONNX: runs/train/soccer_players_clean-exp/weights/best.onnx\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"runs/train/soccer_players_clean-exp/weights/best.pt\")\n",
    "\n",
    "onnx_path = model.export(format=\"onnx\", imgsz=IMG_SIZE, opset=12)\n",
    "print(\"Exported ONNX:\", onnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b506ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX file exported successfully: /home/surya/c/runs/train/soccer_players_clean-exp/weights/best.onnx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "onnx_file = Path(onnx_path)\n",
    "if onnx_file.exists():\n",
    "    print(\"ONNX file exported successfully:\", onnx_file.resolve())\n",
    "else:\n",
    "    print(\"ONNX export failed â€” file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7d2aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration images prepared: 0 at /home/surya/c/hailo/calib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil, random\n",
    "\n",
    "HAILO_DIR = Path.cwd() / \"hailo\"\n",
    "CALIB_DIR = HAILO_DIR / \"calib\"\n",
    "CALIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "all_imgs = [p for p in train_images.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "random.seed(42); random.shuffle(all_imgs)\n",
    "\n",
    "# Choose up to 1000 diverse images for calibration\n",
    "pick = all_imgs[:1000] if len(all_imgs) > 1000 else all_imgs\n",
    "for p in pick:\n",
    "    shutil.copy2(p, CALIB_DIR / p.name)\n",
    "\n",
    "print(f\"Calibration images prepared: {len(pick)} at {CALIB_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60a3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: /home/surya/c/datasets/soccer players.v1i.yolov8/../train/images exists: False\n",
      "val_images  : /home/surya/c/datasets/soccer players.v1i.yolov8/../valid/images exists: False\n",
      "test_images : /home/surya/c/datasets/soccer players.v1i.yolov8/../test/images exists: False\n",
      "All images under DATA_ROOT: 163 (first 10 shown)\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00278_jpeg_jpg.rf.076b339a54a17d03949edac9cee5e9eb.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00356_jpeg_jpg.rf.fdc37ae2fd1a1854d7740feb20a72b96.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00009_jpeg_jpg.rf.4e13eea75a7acdd1eb847744fdff39c4.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00718_jpeg_jpg.rf.d3780ffd86a6d5db62974943c65d43ac.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00038_jpeg_jpg.rf.c3b4d342523fe527e69b9c11b579c184.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00198_jpeg_jpg.rf.7f58d29ba8106080c803610283c09099.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00006_jpeg_jpg.rf.e687460a906357f97cfcd1d2fc481219.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/1-fps-2_00021_jpeg_jpg.rf.fef13e93a5c4188c9e24589054f4d8fd.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/yt1s-com-Resume-Tottenham-20-West-Bromwich-Premier-League-J23_720p-fps-2_00005_jpeg_jpg.rf.5898936ac5c07cb4d6bcb00464d9fac1.jpg\n",
      "   /home/surya/c/datasets/soccer players.v1i.yolov8/train/images/yt1s-com-Resume-Liverpool-01-Burnley-Premier-League-J18-fps-2_00059_jpeg_jpg.rf.0defe4688956fabe9195f423d1775692.jpg\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"train_images:\", train_images, \"exists:\", train_images.exists())\n",
    "print(\"val_images  :\", val_images,   \"exists:\", val_images.exists())\n",
    "print(\"test_images :\", test_images,  \"exists:\", test_images.exists())\n",
    "\n",
    "# Count by split and show a few examples\n",
    "def count_and_sample(folder: Path, max_show=10):\n",
    "    if not folder or not folder.exists():\n",
    "        return 0, []\n",
    "    exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".JPG\",\".PNG\",\".JPEG\",\".BMP\",\".TIF\",\".TIFF\"}\n",
    "    imgs = [p for p in folder.rglob(\"*\") if p.suffix in exts]\n",
    "    print(f\"{folder} -> {len(imgs)} images\")\n",
    "    for p in imgs[:max_show]:\n",
    "        print(\"  \", p)\n",
    "    return len(imgs), imgs[:max_show]\n",
    "\n",
    "n_train, _ = count_and_sample(train_images)\n",
    "n_val,   _ = count_and_sample(val_images)\n",
    "n_test,  _ = count_and_sample(test_images)\n",
    "\n",
    "# Fallback: scan the whole dataset root (in case YAML paths are odd)\n",
    "DATA_ROOT\n",
    "all_any = [p for p in DATA_ROOT.rglob(\"*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}]\n",
    "print(f\"All images under DATA_ROOT: {len(all_any)} (first 10 shown)\")\n",
    "for p in all_any[:10]:\n",
    "    print(\"  \", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62dcab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No split images found; falling back to scan entire DATA_ROOT.\n",
      "Calibration images prepared: 163 at /home/surya/c/hailo/calib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil, random\n",
    "\n",
    "HAILO_DIR = Path.cwd() / \"hailo\"\n",
    "CALIB_DIR = HAILO_DIR / \"calib\"\n",
    "CALIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect from train, val, test in that order; if none, fall back to any under DATA_ROOT\n",
    "exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".JPG\",\".PNG\",\".JPEG\",\".BMP\",\".TIF\",\".TIFF\"}\n",
    "\n",
    "def gather(folder: Path):\n",
    "    return [p for p in folder.rglob(\"*\") if p.suffix in exts] if folder and folder.exists() else []\n",
    "\n",
    "pool = []\n",
    "for folder in [train_images, val_images, test_images]:\n",
    "    pool.extend(gather(folder))\n",
    "\n",
    "if not pool:\n",
    "    print(\"No split images found; falling back to scan entire DATA_ROOT.\")\n",
    "    pool = [p for p in DATA_ROOT.rglob(\"*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}]\n",
    "\n",
    "# Deduplicate by name (to avoid overwrites); if duplicates, append an index\n",
    "random.seed(42)\n",
    "random.shuffle(pool)\n",
    "\n",
    "# Cap at ~1000 images for calibration (use fewer if dataset is small)\n",
    "cap = min(1000, len(pool))\n",
    "picked = pool[:cap]\n",
    "\n",
    "# Ensure unique filenames in CALIB_DIR\n",
    "used = set()\n",
    "copied = 0\n",
    "for i, src in enumerate(picked):\n",
    "    base = src.name\n",
    "    name = base\n",
    "    stem, suf = src.stem, src.suffix\n",
    "    k = 1\n",
    "    while name in used or (CALIB_DIR / name).exists():\n",
    "        name = f\"{stem}_{k}{suf}\"\n",
    "        k += 1\n",
    "    used.add(name)\n",
    "    shutil.copy2(src, CALIB_DIR / name)\n",
    "    copied += 1\n",
    "\n",
    "print(f\"Calibration images prepared: {copied} at {CALIB_DIR}\")\n",
    "if copied == 0:\n",
    "    print(\"âš ï¸ Still zero. Check that DATA_ROOT is correct and that images actually exist on disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7e0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Webcam open. Press 'q' to quit.  Live controls: -, =, t, r, p\n",
      "âœ… Webcam released cleanly\n"
     ]
    }
   ],
   "source": [
    "# Webcam real-time tester for YOLOv8 (with live controls, FPS, recording)\n",
    "# Keys:\n",
    "#   q  = quit (releases camera cleanly)\n",
    "#   -  = decrease confidence threshold (e.g., 0.50 â†’ 0.40 â†’ 0.30 â€¦)\n",
    "#   =  = increase confidence threshold\n",
    "#   t  = toggle TTA (test-time augmentation)  [off by default; costs FPS]\n",
    "#   r  = start/stop recording annotated video to ./runs/webcam/\n",
    "#   p  = pause/resume\n",
    "#\n",
    "# Tips for screen-capture scenarios: use higher imgsz (960/1280), lower conf (0.10â€“0.35),\n",
    "# darken room to reduce glare, square the webcam to the display to avoid perspective skew.\n",
    "\n",
    "import cv2, time, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "MODEL_PATH = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "CAM_INDEX  = 0            # default webcam\n",
    "IMG_SIZE   = 960          # 640 ok, but 960/1280 helps small players/ball\n",
    "CONF       = 0.25         # start stricter; lower live with '-' key if nothing appears\n",
    "IOU        = 0.60\n",
    "USE_TTA    = False        # toggle live with 't'\n",
    "DISPLAY_W, DISPLAY_H = None, None  # set e.g. (1280, 720) to force viewer size\n",
    "# =====================\n",
    "\n",
    "# Load model once\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(CAM_INDEX)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"âŒ Could not open webcam index {CAM_INDEX}\")\n",
    "\n",
    "# (Optional) try to set higher capture resolution for cleaner frames\n",
    "try:\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Recording\n",
    "save_root = Path(\"runs/webcam\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "writer = None\n",
    "recording = False\n",
    "\n",
    "# FPS smoothing\n",
    "t0 = time.time()\n",
    "frame_count = 0\n",
    "fps_ema = None\n",
    "paused = False\n",
    "\n",
    "def start_writer(frame, root):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = root / f\"webcam_{ts}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    h, w = frame.shape[:2]\n",
    "    w_out, h_out = (DISPLAY_W or w), (DISPLAY_H or h)\n",
    "    vw = cv2.VideoWriter(str(out_path), fourcc, 30.0, (w_out, h_out))\n",
    "    return vw, out_path\n",
    "\n",
    "print(\"âœ… Webcam open. Press 'q' to quit.  Live controls: -, =, t, r, p\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if not paused:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                print(\"âš ï¸ Frame grab failed; stopping.\")\n",
    "                break\n",
    "\n",
    "            # Inference (Ultralytics handles letterboxing internally)\n",
    "            # Use direct call `model(frame, ...)` for lower overhead than .predict on streams\n",
    "                annotated = results[0].plot()  # draw boxes, labels, conf\n",
    "\n",
    "            # FPS\n",
    "            frame_count += 1\n",
    "            if frame_count >= 1:\n",
    "                dt = time.time() - t0\n",
    "                inst_fps = frame_count / max(dt, 1e-9)\n",
    "                fps_ema = inst_fps if fps_ema is None else (0.9 * fps_ema + 0.1 * inst_fps)\n",
    "\n",
    "            # Overlay status text\n",
    "            overlay = annotated\n",
    "            h, w = overlay.shape[:2]\n",
    "            if DISPLAY_W and DISPLAY_H:\n",
    "                overlay = cv2.resize(overlay, (DISPLAY_W, DISPLAY_H))\n",
    "\n",
    "            hud = f\"FPS: {fps_ema:.1f} | imgsz: {IMG_SIZE} | conf: {CONF:.2f} | iou: {IOU:.2f} | TTA: {USE_TTA} | rec: {recording}\"\n",
    "            cv2.putText(overlay, hud, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            overlay = cv2.putText(overlay, \"PAUSED (press 'p' to resume)\", (10, 60),\n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show\n",
    "        cv2.imshow(\"YOLOv8 Webcam (press q to quit)\", overlay)\n",
    "\n",
    "        # Write video if recording\n",
    "        if recording:\n",
    "            if writer is None:\n",
    "                writer, current_out = start_writer(overlay, save_root)\n",
    "                print(f\"âºï¸ Recording to: {current_out}\")\n",
    "            writer.write(overlay)\n",
    "        elif writer is not None:\n",
    "            writer.release()\n",
    "            print(\"â¹ï¸ Recording stopped.\")\n",
    "            writer = None\n",
    "\n",
    "        # Keys\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "        elif k == ord('-'):\n",
    "            CONF = max(0.05, CONF - 0.05)\n",
    "        elif k == ord('=') or k == ord('+'):\n",
    "            CONF = min(0.95, CONF + 0.05)\n",
    "        elif k == ord('t'):\n",
    "            USE_TTA = not USE_TTA\n",
    "        elif k == ord('r'):\n",
    "            recording = not recording\n",
    "        elif k == ord('p'):\n",
    "            paused = not paused\n",
    "        elif k == ord('['):\n",
    "            IMG_SIZE = max(320, IMG_SIZE - 64)\n",
    "        elif k == ord(']'):\n",
    "            IMG_SIZE = min(1536, IMG_SIZE + 64)\n",
    "\n",
    "finally:\n",
    "    # Clean release\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"âœ… Webcam released cleanly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a9dc927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FFmpeg at: /home/surya/c/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2\n",
      "ffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n"
     ]
    }
   ],
   "source": [
    "import sys, os, shutil, subprocess, stat\n",
    "!{sys.executable} -m pip -q install yt-dlp imageio-ffmpeg ultralytics opencv-python\n",
    "\n",
    "# Get a real ffmpeg binary (bundled) and ensure it's executable, then add to PATH\n",
    "import imageio_ffmpeg\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "# Make executable (Linux/macOS safety)\n",
    "try:\n",
    "    os.chmod(ffmpeg_path, os.stat(ffmpeg_path).st_mode | stat.S_IEXEC)\n",
    "except Exception as e:\n",
    "    print(\"chmod skipped:\", e)\n",
    "\n",
    "ffmpeg_dir = os.path.dirname(ffmpeg_path)\n",
    "os.environ[\"PATH\"] = ffmpeg_dir + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Sanity: show ffmpeg version from this binary\n",
    "print(\"Using FFmpeg at:\", ffmpeg_path)\n",
    "print(subprocess.check_output([ffmpeg_path, \"-version\"], text=True).splitlines()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a2725d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[av1 @ 0x404d5b00] Your platform doesn't suppport hardware accelerated AV1 decoding.\n",
      "[av1 @ 0x404d5b00] Failed to get pixel format.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x404d5b00] Missing Sequence Header.\n",
      "[av1 @ 0x43a78740] Your platform doesn't suppport hardware accelerated AV1 decoding.\n",
      "[av1 @ 0x43a78740] Failed to get pixel format.\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# Download video from VIDEO_URL\n",
    "def download_video(url, out_path=\"input_video.mp4\"):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo+bestaudio/best',\n",
    "        'outtmpl': out_path,\n",
    "        'quiet': True,\n",
    "        'merge_output_format': 'mp4',\n",
    "        # Use the ffmpeg binary already installed\n",
    "        # This uses the ffmpeg_path variable set earlier\n",
    "        # If you want to force re-encode to H.264 for compatibility, add postprocessors:\n",
    "        # 'postprocessors': [{'key': 'FFmpegVideoConvertor', 'preferedformat': 'mp4'}],\n",
    "        'ffmpeg_location': ffmpeg_path,\n",
    "        'postprocessors': [{\n",
    "                'key': 'FFmpegVideoConvertor',\n",
    "                'preferedformat': 'mp4'\n",
    "        }],\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "    return out_path\n",
    "\n",
    "video_path = download_video(\"https://www.youtube.com/watch?v=-D5lO5Pl-3Q\")\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    results = model(frame, imgsz=IMG_SIZE, conf=CONF, iou=IOU, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "    cv2.imshow(\"YOLOv8 Football Detection\", annotated)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd3cf229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg: ffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n"
     ]
    }
   ],
   "source": [
    "import sys, os, stat, subprocess, shutil\n",
    "!{sys.executable} -m pip -q install yt-dlp imageio-ffmpeg ultralytics opencv-python\n",
    "\n",
    "# Get a real ffmpeg binary and put it on PATH\n",
    "import imageio_ffmpeg\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "try:\n",
    "    os.chmod(ffmpeg_path, os.stat(ffmpeg_path).st_mode | stat.S_IEXEC)\n",
    "except Exception:\n",
    "    pass\n",
    "os.environ[\"PATH\"] = os.path.dirname(ffmpeg_path) + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "\n",
    "print(\"FFmpeg:\", subprocess.check_output([ffmpeg_path, \"-version\"], text=True).splitlines()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "578db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "def download_video_h264(url: str, out_dir=\"downloads\") -> Path:\n",
    "    \"\"\"\n",
    "    Download video preferring avc1(H.264)+m4a(AAC) MP4. Falls back to best if needed.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        \"outtmpl\": str(out_dir / \"%(title).90s.%(ext)s\"),\n",
    "        # Prefer MP4 + H.264 video + m4a audio; then any mp4; else 'best'\n",
    "        \"format\": (\n",
    "            \"bestvideo[ext=mp4][vcodec^=avc1]+bestaudio[ext=m4a]/\"\n",
    "            \"best[ext=mp4][vcodec^=avc1]/\"\n",
    "            \"bestvideo[vcodec^=avc1]+bestaudio/\"\n",
    "            \"best\"\n",
    "        ),\n",
    "        \"merge_output_format\": \"mp4\",\n",
    "        \"ffmpeg_location\": ffmpeg_path,   # point yt-dlp to the actual ffmpeg binary\n",
    "        \"noplaylist\": True,\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        fn = Path(ydl.prepare_filename(info))\n",
    "        mp4 = fn.with_suffix(\".mp4\")\n",
    "        return mp4 if mp4.exists() else fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "259bb300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex, cv2\n",
    "\n",
    "def ensure_readable_mp4(src: Path, force_reencode=False) -> Path:\n",
    "    \"\"\"\n",
    "    Make sure OpenCV can open the file. If not, or if force_reencode=True,\n",
    "    transcode to H.264 (libx264) + AAC in an MP4 container.\n",
    "    \"\"\"\n",
    "    src = Path(src)\n",
    "\n",
    "    def can_open(p: Path) -> bool:\n",
    "        cap = cv2.VideoCapture(str(p), cv2.CAP_FFMPEG)\n",
    "        ok = cap.isOpened()\n",
    "        cap.release()\n",
    "        return ok\n",
    "\n",
    "    if not force_reencode and can_open(src):\n",
    "        return src\n",
    "\n",
    "    # Try fast remux first\n",
    "    remux = src.with_suffix(\".remux.mp4\")\n",
    "    cmd = [ffmpeg_path, \"-y\", \"-i\", str(src), \"-c\", \"copy\", \"-movflags\", \"+faststart\", str(remux)]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, capture_output=True)\n",
    "        if not force_reencode and can_open(remux):\n",
    "            return remux\n",
    "    except subprocess.CalledProcessError:\n",
    "        pass\n",
    "\n",
    "    # Re-encode to H.264/AAC (yuv420p = widely compatible)\n",
    "    reenc = src.with_suffix(\".h264.mp4\")\n",
    "    cmd = [\n",
    "        ffmpeg_path, \"-y\", \"-i\", str(src),\n",
    "        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
    "        \"-c:a\", \"aac\", \"-movflags\", \"+faststart\",\n",
    "        str(reenc)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    if not can_open(reenc):\n",
    "        raise RuntimeError(\"Re-encoded MP4 still unreadable by OpenCV.\")\n",
    "    return reenc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362460fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (preferring H.264)â€¦\n",
      "Saved: downloads/FC Dallas vs. LAFC ï½œ Full Match Highlights ï½œ Son Heung-Min BANGER!!.mp4\n",
      "Ensuring OpenCV-readable MP4â€¦\n",
      "Using: downloads/FC Dallas vs. LAFC ï½œ Full Match Highlights ï½œ Son Heung-Min BANGER!!.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2, time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "VIDEO_URL   = \"https://www.youtube.com/watch?v=-D5lO5Pl-3Q\"\n",
    "MODEL_PATH  = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "CONF        = 0.20       # try 0.10â€“0.35 if detections are weak\n",
    "IOU         = 0.60\n",
    "IMG_SIZE    = 960        # 960/1280 help with small players/ball\n",
    "USE_TTA     = False\n",
    "SHOW_WINDOW = False\n",
    "OUT_DIR     = Path(\"runs/video_pred\")\n",
    "# =====================\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Download with avc1 preference\n",
    "print(\"Downloading (preferring H.264)â€¦\")\n",
    "local_file = download_video_h264(VIDEO_URL)\n",
    "print(\"Saved:\", local_file)\n",
    "\n",
    "# 2) Ensure OpenCV can read (remux/re-encode if needed)\n",
    "print(\"Ensuring OpenCV-readable MP4â€¦\")\n",
    "readable = ensure_readable_mp4(local_file)\n",
    "print(\"Using:\", readable)\n",
    "\n",
    "# 3) Open and annotate\n",
    "cap = cv2.VideoCapture(str(readable), cv2.CAP_FFMPEG)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Failed to open video after conversion.\")\n",
    "\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 1280)\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 720)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out_path = OUT_DIR / f\"annotated_{int(time.time())}.mp4\"\n",
    "writer = cv2.VideoWriter(str(out_path), fourcc, fps, (width, height))\n",
    "if not writer.isOpened():\n",
    "    raise RuntimeError(\"VideoWriter failed; check codecs.\")\n",
    "\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "t0 = time.time(); frames = 0\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    results = model(frame, conf=CONF, iou=IOU, imgsz=IMG_SIZE, augment=USE_TTA, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "\n",
    "    frames += 1\n",
    "    if frames % 10 == 0:\n",
    "        fps_cur = frames / (time.time() - t0 + 1e-9)\n",
    "        cv2.putText(annotated, f\"FPS~{fps_cur:.1f} conf:{CONF:.2f} imgsz:{IMG_SIZE} TTA:{USE_TTA}\",\n",
    "                    (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    writer.write(annotated)\n",
    "    if SHOW_WINDOW:\n",
    "        cv2.imshow(\"YOLOv8 Video\", annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "if SHOW_WINDOW:\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"âœ… Saved annotated video to:\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4937f327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Detections:\n",
      " - 0: 0.48\n",
      " - 0: 0.47\n",
      " - 0: 0.47\n",
      "ðŸ’¾ Saved annotated image to: /home/surya/c/runs/detect/single/OIP.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/surya/c/.venv/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ===== EDIT =====\n",
    "MODEL_PATH = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "IMG_SIZE   = 960\n",
    "CONF       = 0.25\n",
    "IOU        = 0.60\n",
    "# ===============\n",
    "\n",
    "def _is_url(s: str) -> bool:\n",
    "    try:\n",
    "        u = urlparse(s)\n",
    "        return u.scheme in (\"http\", \"https\") and u.netloc != \"\"\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _load_image(url_or_path: str):\n",
    "    s = str(url_or_path).strip()\n",
    "\n",
    "    # Common mistake: \"/https://...\" -> strip leading slashes\n",
    "    if s.startswith(\"/http\"):\n",
    "        s = s.lstrip(\"/\")\n",
    "\n",
    "    if _is_url(s):\n",
    "        # Download with a browser-like UA; some CDNs reject default Python UA\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        r = requests.get(s, headers=headers, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        arr = np.frombuffer(r.content, dtype=np.uint8)\n",
    "        img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise RuntimeError(\"Downloaded bytes could not be decoded as an image.\")\n",
    "        return img, s\n",
    "    else:\n",
    "        img = cv2.imread(s)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Could not read image from path: {s}\")\n",
    "        return img, s\n",
    "\n",
    "def detect_image(url_or_path, show=True, save_path=None):\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "    img, src = _load_image(url_or_path)\n",
    "    results = model(img, imgsz=IMG_SIZE, conf=CONF, iou=IOU, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "\n",
    "    # Print detections\n",
    "    boxes = results[0].boxes\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        print(\"âŒ No detections.\")\n",
    "    else:\n",
    "        print(\"âœ… Detections:\")\n",
    "        for b in boxes:\n",
    "            cls = int(b.cls[0])\n",
    "            conf = float(b.conf[0])\n",
    "            name = model.names.get(cls, str(cls)) if hasattr(model, \"names\") else str(cls)\n",
    "            print(f\" - {name}: {conf:.2f}\")\n",
    "\n",
    "    # Decide output path\n",
    "    if save_path is None:\n",
    "        # derive a filename from the URL/path (fallback to result.jpg)\n",
    "        parsed = urlparse(src)\n",
    "        fname = Path(parsed.path).name or \"result.jpg\"\n",
    "        out = Path(\"runs/detect/single\") / fname\n",
    "        out = out.with_suffix(\".jpg\")  # make sure it's an image extension\n",
    "        out.parent.mkdir(parents=True, exist_ok=True)\n",
    "        save_path = out\n",
    "\n",
    "    cv2.imwrite(str(save_path), annotated)\n",
    "    print(\"ðŸ’¾ Saved annotated image to:\", save_path.resolve())\n",
    "\n",
    "    if show:\n",
    "        cv2.imshow(\"YOLOv8 Detection\", annotated)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# -------- Example usage (NOTICE: no leading slash) --------\n",
    "detect_image(\"https://tse3.mm.bing.net/th/id/OIP.nvBzlZT0DJGp0ey4-bmUCwAAAA?rs=1&pid=ImgDetMain&o=7&rm=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaadffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image: tmp_url.jpg\n",
      "Results saved to \u001b[1mruns/detect/predict4\u001b[0m\n",
      "[conf=0.05] detections=7  -> saved at: runs/detect/predict4\n",
      "  box: 0 0.39\n",
      "  box: 0 0.21\n",
      "  box: 0 0.17\n",
      "  box: 0 0.15\n",
      "  box: 0 0.09\n",
      "  box: 1 0.07\n",
      "  box: 1 0.05\n",
      "\n",
      "Open the newest file inside: runs/detect/predict4\n",
      "   runs/detect/predict4/tmp_url.jpg\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e152391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 8 training images\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00002_jpeg_jpg.rf.05c4a23f214f0186a4f41642c287200c.jpg -> 21 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00006_jpeg_jpg.rf.e687460a906357f97cfcd1d2fc481219.jpg -> 16 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00008_jpeg_jpg.rf.14e0ee0c5d38ba5fc78402e000e3b66c.jpg -> 9 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00009_jpeg_jpg.rf.4e13eea75a7acdd1eb847744fdff39c4.jpg -> 9 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00010_jpeg_jpg.rf.98f0fb890d12f95d8ca1d47921805b30.jpg -> 9 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00011_jpeg_jpg.rf.4bbffbe52093c2d7698d8115eee14ae2.jpg -> 10 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00012_jpeg_jpg.rf.bc0c636c0c9a533aa6974eabb0f4b4eb.jpg -> 8 detections; saved: runs/detect/predict5\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "1-fps-2_00021_jpeg_jpg.rf.fef13e93a5c4188c9e24589054f4d8fd.jpg -> 15 detections; saved: runs/detect/predict5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_PATH = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "TRAIN_IMG_DIR = Path(\"/home/surya/Downloads/soccer players.v1i.yolov8/train/images\")  # adjust if needed\n",
    "\n",
    "model = YOLO(MODEL_PATH)\n",
    "imgs = sorted([p for p in TRAIN_IMG_DIR.glob(\"*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}])[:8]\n",
    "print(\"Testing on\", len(imgs), \"training images\")\n",
    "\n",
    "for p in imgs:\n",
    "    res = model.predict(source=str(p), imgsz=1280, conf=0.10, iou=0.60, save=True, show=False, verbose=False)\n",
    "    n = 0 if res[0].boxes is None else len(res[0].boxes)\n",
    "    print(p.name, \"->\", n, \"detections; saved:\", res[0].save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6092e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image: tmp_url.jpg\n",
      "Results saved to \u001b[1mruns/detect/predict6\u001b[0m\n",
      "[conf=0.05] detections=7  -> saved at: runs/detect/predict6\n",
      "  box: 0 0.39\n",
      "  box: 0 0.21\n",
      "  box: 0 0.17\n",
      "  box: 0 0.15\n",
      "  box: 0 0.09\n",
      "  box: 1 0.07\n",
      "  box: 1 0.05\n",
      "\n",
      "Open the newest file inside: runs/detect/predict6\n",
      "   runs/detect/predict6/tmp_url.jpg\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_PATH = \"runs/train/soccer_players_clean-exp/weights/best.pt\"  # your trained model\n",
    "IMG_SIZE   = 1280  # bigger helps tiny players/ball\n",
    "IOU        = 0.60\n",
    "\n",
    "def fetch_local(src: str) -> str:\n",
    "    s = src.strip()\n",
    "    if s.startswith(\"/http\"):  # common typo fix\n",
    "        s = s.lstrip(\"/\")\n",
    "    if s.startswith(\"http://\") or s.startswith(\"https://\"):\n",
    "        r = requests.get(s, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        fn = Path(\"tmp_url.jpg\")\n",
    "        fn.write_bytes(r.content)\n",
    "        return str(fn)\n",
    "    return s\n",
    "\n",
    "def test_image(src: str):\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    local = fetch_local(src)\n",
    "    print(\"Using image:\", local)\n",
    "\n",
    "    # Try a ladder of conf thresholds to avoid â€œ0 detectionsâ€ hiding boxes\n",
    "    tried = []\n",
    "    for conf in [0.05, 0.10, 0.15, 0.20, 0.25]:\n",
    "        res = model.predict(\n",
    "            source=local,\n",
    "            imgsz=IMG_SIZE,\n",
    "            conf=conf,\n",
    "            iou=IOU,\n",
    "            agnostic_nms=True,\n",
    "            max_det=300,\n",
    "            save=True,     # <-- saves annotated image/video\n",
    "            show=False,\n",
    "            verbose=False\n",
    "        )\n",
    "        n = 0 if res[0].boxes is None else len(res[0].boxes)\n",
    "        out_dir = res[0].save_dir\n",
    "        tried.append((conf, n, out_dir))\n",
    "        print(f\"[conf={conf:.2f}] detections={n}  -> saved at: {out_dir}\")\n",
    "\n",
    "        # If anything was detected, stop early\n",
    "        if n > 0:\n",
    "            # Also print the detections\n",
    "            for b in res[0].boxes:\n",
    "                cls = int(b.cls[0]) if b.cls is not None else -1\n",
    "                confv = float(b.conf[0])\n",
    "                print(\"  box:\", res[0].names.get(cls, str(cls)), f\"{confv:.2f}\")\n",
    "            break\n",
    "\n",
    "    # Tell user where to look\n",
    "    last_dir = tried[-1][2]\n",
    "    print(\"\\nOpen the newest file inside:\", last_dir)\n",
    "    for p in sorted(Path(last_dir).glob(\"*\"))[-5:]:\n",
    "        print(\"  \", p)\n",
    "\n",
    "# EXAMPLE (put your URL here)\n",
    "test_image(\"https://tse3.mm.bing.net/th/id/OIP.nvBzlZT0DJGp0ey4-bmUCwAAAA?rs=1&pid=ImgDetMain&o=7&rm=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa48f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying direct progressive stream â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Streaming directly.\n",
      "Opened stream @ ~30.0 FPS\n",
      "âœ… Released resources\n"
     ]
    }
   ],
   "source": [
    "# Real-time detection from a VIDEO URL using your existing YOLO model (version-safe: no 'tta' arg)\n",
    "# Hotkeys:\n",
    "#   q  = quit\n",
    "#   -  = lower confidence\n",
    "#   =  = raise confidence\n",
    "#   [  = smaller imgsz\n",
    "#   ]  = larger imgsz\n",
    "#   a  = toggle inference-time augmentation (if your Ultralytics build supports 'augment')\n",
    "\n",
    "import os, sys, time, stat, subprocess\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ============ EDIT THESE ============\n",
    "VIDEO_URL  = \"https://www.youtube.com/watch?v=-D5lO5Pl-3Q\"   # your link\n",
    "MODEL_PATH = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "CONF       = 0.20\n",
    "IOU        = 0.60\n",
    "IMG_SIZE   = 960\n",
    "USE_AUGMENT = False   # toggled with 'a'\n",
    "SHOW       = True\n",
    "# ====================================\n",
    "\n",
    "# --- Ensure a real ffmpeg is available ---\n",
    "import imageio_ffmpeg\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "try:\n",
    "    os.chmod(ffmpeg_path, os.stat(ffmpeg_path).st_mode | stat.S_IEXEC)\n",
    "except Exception:\n",
    "    pass\n",
    "os.environ[\"PATH\"] = os.path.dirname(ffmpeg_path) + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "\n",
    "# --- yt-dlp helpers: direct progressive stream if possible; else download/convert ---\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "def get_direct_stream(url: str):\n",
    "    ydl_opts = {\"quiet\": True, \"noplaylist\": True}\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        fmts = info.get(\"formats\", [])\n",
    "        cand = [f for f in fmts\n",
    "                if f.get(\"ext\")==\"mp4\"\n",
    "                and (f.get(\"vcodec\",\"\").startswith(\"avc1\"))\n",
    "                and (f.get(\"acodec\",\"none\") != \"none\")\n",
    "                and f.get(\"url\")]\n",
    "        if not cand:\n",
    "            return None\n",
    "        cand.sort(key=lambda f: (f.get(\"height\") or 0), reverse=True)\n",
    "        return cand[0][\"url\"]\n",
    "\n",
    "def download_video_h264(url: str, out_dir=\"downloads\") -> Path:\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        \"outtmpl\": str(out_dir / \"%(title).90s.%(ext)s\"),\n",
    "        \"format\": (\n",
    "            \"bestvideo[ext=mp4][vcodec^=avc1]+bestaudio[ext=m4a]/\"\n",
    "            \"best[ext=mp4][vcodec^=avc1]/\"\n",
    "            \"best\"\n",
    "        ),\n",
    "        \"merge_output_format\": \"mp4\",\n",
    "        \"ffmpeg_location\": os.path.dirname(ffmpeg_path),\n",
    "        \"noplaylist\": True,\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        fn = Path(ydl.prepare_filename(info))\n",
    "        mp4 = fn.with_suffix(\".mp4\")\n",
    "        return mp4 if mp4.exists() else fn\n",
    "\n",
    "def ensure_readable_mp4(src: Path) -> Path:\n",
    "    def can_open(p: Path) -> bool:\n",
    "        cap = cv2.VideoCapture(str(p), cv2.CAP_FFMPEG)\n",
    "        ok = cap.isOpened(); cap.release()\n",
    "        return ok\n",
    "    if can_open(src):\n",
    "        return src\n",
    "    # remux\n",
    "    remux = src.with_suffix(\".remux.mp4\")\n",
    "    subprocess.run([ffmpeg_path, \"-y\", \"-i\", str(src), \"-c\", \"copy\",\n",
    "                    \"-movflags\", \"+faststart\", str(remux)],\n",
    "                   check=False, capture_output=True, text=True)\n",
    "    if can_open(remux):\n",
    "        return remux\n",
    "    # re-encode\n",
    "    reenc = src.with_suffix(\".h264.mp4\")\n",
    "    subprocess.run([ffmpeg_path, \"-y\", \"-i\", str(src),\n",
    "                    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
    "                    \"-c:a\", \"aac\", \"-movflags\", \"+faststart\", str(reenc)],\n",
    "                   check=True)\n",
    "    if not can_open(reenc):\n",
    "        raise RuntimeError(\"Re-encoded MP4 still unreadable by OpenCV.\")\n",
    "    return reenc\n",
    "\n",
    "def open_video_from_url(url: str):\n",
    "    print(\"Trying direct progressive stream â€¦\")\n",
    "    direct = None\n",
    "    try:\n",
    "        direct = get_direct_stream(url)\n",
    "    except Exception as e:\n",
    "        print(\"Direct stream lookup failed:\", e)\n",
    "    if direct:\n",
    "        cap = cv2.VideoCapture(direct, cv2.CAP_FFMPEG)\n",
    "        if cap.isOpened():\n",
    "            print(\"âœ” Streaming directly.\")\n",
    "            return cap\n",
    "        cap.release()\n",
    "    print(\"âš  Streaming failed â†’ downloading then playing â€¦\")\n",
    "    local = download_video_h264(url)\n",
    "    print(\"Downloaded to:\", local)\n",
    "    playable = ensure_readable_mp4(local)\n",
    "    print(\"Using:\", playable)\n",
    "    cap = cv2.VideoCapture(str(playable), cv2.CAP_FFMPEG)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Failed to open video even after conversion.\")\n",
    "    return cap\n",
    "\n",
    "# --- Realtime loop with version-safe inference call (no 'tta') ---\n",
    "model = YOLO(MODEL_PATH)\n",
    "cap = open_video_from_url(VIDEO_URL)\n",
    "fps0 = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "print(f\"Opened stream @ ~{fps0:.1f} FPS\")\n",
    "\n",
    "t0 = time.time(); frames = 0\n",
    "try:\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # Version-safe inference: try 'augment' if supported, else plain call\n",
    "        try:\n",
    "            results = model(frame, conf=CONF, iou=IOU, imgsz=IMG_SIZE,\n",
    "                            augment=USE_AUGMENT, verbose=False)\n",
    "        except TypeError:\n",
    "            results = model(frame, conf=CONF, iou=IOU, imgsz=IMG_SIZE, verbose=False)\n",
    "\n",
    "        annotated = results[0].plot()\n",
    "\n",
    "        frames += 1\n",
    "        if frames % 10 == 0:\n",
    "            fps_cur = frames / (time.time() - t0 + 1e-9)\n",
    "            cv2.putText(annotated, f\"FPS~{fps_cur:.1f} conf:{CONF:.2f} imgsz:{IMG_SIZE} augment:{USE_AUGMENT}\",\n",
    "                        (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if SHOW:\n",
    "            cv2.imshow(\"YOLOv8 URL real-time\", annotated)\n",
    "\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "        elif k == ord('-'):\n",
    "            CONF = max(0.05, CONF - 0.05)\n",
    "        elif k in (ord('='), ord('+')):\n",
    "            CONF = min(0.95, CONF + 0.05)\n",
    "        elif k == ord('['):\n",
    "            IMG_SIZE = max(320, IMG_SIZE - 64)\n",
    "        elif k == ord(']'):\n",
    "            IMG_SIZE = min(1536, IMG_SIZE + 64)\n",
    "        elif k == ord('a'):\n",
    "            USE_AUGMENT = not USE_AUGMENT\n",
    "            print(\"augment =\", USE_AUGMENT)\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"âœ… Released resources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f7074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available; running on CPU.\n",
      "Trying direct progressive stream â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Streaming 360p progressive MP4.\n",
      "Opened 640x360 @ ~30.0 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/surya/c/.venv/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Released resources\n"
     ]
    }
   ],
   "source": [
    "# Smooth real-time detection from VIDEO URL (version-safe, faster)\n",
    "# Hotkeys:\n",
    "#   q = quit\n",
    "#   - / = : lower/raise confidence\n",
    "#   [ / ] : smaller/larger imgsz\n",
    "#   a     : toggle inference-time augment (off by default; slower)\n",
    "#   n / m : process every Nth frame (â†“ or â†‘)  e.g., n=2 ~ process 15 of 30 FPS\n",
    "\n",
    "import os, time, stat, subprocess\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "VIDEO_URL   = \"https://www.youtube.com/watch?v=-D5lO5Pl-3Q\"\n",
    "MODEL_PATH  = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "CONF        = 0.20\n",
    "IOU         = 0.60\n",
    "IMG_SIZE    = 960          # 640 is faster; 960/1280 sees smaller players\n",
    "USE_AUGMENT = False        # keep False for speed\n",
    "PROCESS_EVERY_N = 2        # process every Nth frame; display all (2 or 3 is a good sweet spot)\n",
    "TARGET_STREAM_HEIGHT = 720 # request 720p progressive for lighter decode\n",
    "SHOW       = True\n",
    "# ============================================\n",
    "\n",
    "# Ensure ffmpeg (bundled) is on PATH\n",
    "import imageio_ffmpeg\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "try: os.chmod(ffmpeg_path, os.stat(ffmpeg_path).st_mode | stat.S_IEXEC)\n",
    "except Exception: pass\n",
    "os.environ[\"PATH\"] = os.path.dirname(ffmpeg_path) + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "\n",
    "# Direct progressive stream finder (H.264 + audio) at or below TARGET_STREAM_HEIGHT\n",
    "from yt_dlp import YoutubeDL\n",
    "def get_direct_stream(url: str, max_h=720):\n",
    "    ydl_opts = {\"quiet\": True, \"noplaylist\": True}\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        fmts = info.get(\"formats\", [])\n",
    "        prog = [f for f in fmts\n",
    "                if f.get(\"ext\")==\"mp4\"\n",
    "                and (f.get(\"vcodec\",\"\").startswith(\"avc1\"))\n",
    "                and (f.get(\"acodec\",\"none\")!=\"none\")\n",
    "                and f.get(\"url\")]\n",
    "        if not prog:\n",
    "            return None\n",
    "        # prefer highest â‰¤ max_h; else lowest above it\n",
    "        below = [f for f in prog if (f.get(\"height\") or 0) <= max_h]\n",
    "        cand = sorted(below, key=lambda f: (f.get(\"height\") or 0), reverse=True) or \\\n",
    "               sorted(prog, key=lambda f: (f.get(\"height\") or 0))\n",
    "        return cand[0][\"url\"], cand[0].get(\"height\")\n",
    "\n",
    "def open_video_from_url(url: str, max_h=720):\n",
    "    print(\"Trying direct progressive stream â€¦\")\n",
    "    direct = None\n",
    "    h_sel  = None\n",
    "    try:\n",
    "        direct, h_sel = get_direct_stream(url, max_h=max_h)\n",
    "    except Exception as e:\n",
    "        print(\"Direct stream lookup failed:\", e)\n",
    "    if direct:\n",
    "        cap = cv2.VideoCapture(direct, cv2.CAP_FFMPEG)\n",
    "        if cap.isOpened():\n",
    "            print(f\"âœ” Streaming {h_sel}p progressive MP4.\")\n",
    "            return cap\n",
    "        cap.release()\n",
    "    # Fallback: download (still works, just less â€œliveâ€)\n",
    "    print(\"âš  Streaming failed â†’ downloading then playing â€¦\")\n",
    "    out_dir = Path(\"downloads\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        \"outtmpl\": str(out_dir / \"%(title).90s.%(ext)s\"),\n",
    "        \"format\": (\n",
    "            \"bestvideo[ext=mp4][vcodec^=avc1][height<=\" + str(max_h) + \"]+bestaudio[ext=m4a]/\"\n",
    "            \"best[ext=mp4][vcodec^=avc1][height<=\" + str(max_h) + \"]/\"\n",
    "            \"best\"\n",
    "        ),\n",
    "        \"merge_output_format\": \"mp4\",\n",
    "        \"ffmpeg_location\": os.path.dirname(ffmpeg_path),\n",
    "        \"noplaylist\": True,\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        fn = Path(ydl.prepare_filename(info)).with_suffix(\".mp4\")\n",
    "    playable = fn\n",
    "    cap = cv2.VideoCapture(str(playable), cv2.CAP_FFMPEG)\n",
    "    if not cap.isOpened():\n",
    "        # last-resort re-encode\n",
    "        reenc = playable.with_suffix(\".h264.mp4\")\n",
    "        subprocess.run([ffmpeg_path, \"-y\", \"-i\", str(playable),\n",
    "                        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
    "                        \"-c:a\", \"aac\", \"-movflags\", \"+faststart\",\n",
    "                        str(reenc)], check=True)\n",
    "        cap = cv2.VideoCapture(str(reenc), cv2.CAP_FFMPEG)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Could not open video.\")\n",
    "    print(\"Playing downloaded MP4.\")\n",
    "    return cap\n",
    "\n",
    "# Speed tips for OpenCV backend\n",
    "try:\n",
    "    cv2.setNumThreads(0)  # avoid oversubscription on some CPUs\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Load your model once; try GPU + FP16 (falls back safely)\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "device_kw = {}\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        device_kw[\"device\"] = 0\n",
    "        # Half precision speeds up a lot on recent GPUs\n",
    "        device_kw[\"half\"] = True\n",
    "        torch.backends.cudnn.benchmark = True  # faster convs for fixed shapes\n",
    "        print(\"Using CUDA + FP16.\")\n",
    "    else:\n",
    "        print(\"CUDA not available; running on CPU.\")\n",
    "except Exception:\n",
    "    print(\"Torch not found or GPU probe failed; proceeding with defaults.\")\n",
    "\n",
    "cap = open_video_from_url(VIDEO_URL, max_h=TARGET_STREAM_HEIGHT)\n",
    "fps0 = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "w0   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 1280)\n",
    "h0   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 720)\n",
    "print(f\"Opened {w0}x{h0} @ ~{fps0:.1f} FPS\")\n",
    "\n",
    "# Processing loop (skim frames for speed)\n",
    "last_annotated = None\n",
    "frame_i = 0\n",
    "t0 = time.time(); shown = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # Always display something, but only run inference every Nth frame\n",
    "        run_infer = (frame_i % max(1, int(PROCESS_EVERY_N)) == 0)\n",
    "        if run_infer or last_annotated is None:\n",
    "            # Version-safe: try augment; otherwise plain call\n",
    "            try:\n",
    "                results = model(frame, conf=CONF, iou=IOU, imgsz=IMG_SIZE,\n",
    "                                augment=USE_AUGMENT, agnostic_nms=True,\n",
    "                                max_det=150, verbose=False, **device_kw)\n",
    "            except TypeError:\n",
    "                results = model(frame, conf=CONF, iou=IOU, imgsz=IMG_SIZE,\n",
    "                                agnostic_nms=True, max_det=150,\n",
    "                                verbose=False, **device_kw)\n",
    "            last_annotated = results[0].plot()\n",
    "\n",
    "        # FPS overlay (display FPS, not inference FPS)\n",
    "        shown += 1\n",
    "        if shown % 10 == 0:\n",
    "            fps_disp = shown / (time.time() - t0 + 1e-9)\n",
    "            cv2.putText(last_annotated, f\"Display~{fps_disp:.1f}  conf:{CONF:.2f}  imgsz:{IMG_SIZE}  N:{PROCESS_EVERY_N}  aug:{USE_AUGMENT}\",\n",
    "                        (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if SHOW:\n",
    "            cv2.imshow(\"YOLOv8 URL realtime (q quit, -/= conf, [/] size, n/m stride, a aug)\", last_annotated)\n",
    "\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == ord('q'): break\n",
    "        elif k == ord('-'): CONF = max(0.01, CONF - 0.05)\n",
    "        elif k in (ord('='), ord('+')): CONF = min(0.99, CONF + 0.05)\n",
    "        elif k == ord('['): IMG_SIZE = max(320, IMG_SIZE - 64)\n",
    "        elif k == ord(']'): IMG_SIZE = min(1536, IMG_SIZE + 64)\n",
    "        elif k == ord('a'): USE_AUGMENT = not USE_AUGMENT; print(\"augment =\", USE_AUGMENT)\n",
    "        elif k == ord('n'): PROCESS_EVERY_N = max(1, PROCESS_EVERY_N + 1); print(\"process every N frames:\", PROCESS_EVERY_N)\n",
    "        elif k == ord('m'): PROCESS_EVERY_N = max(1, PROCESS_EVERY_N - 1); print(\"process every N frames:\", PROCESS_EVERY_N)\n",
    "\n",
    "        frame_i += 1\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"âœ… Released resources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1072ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available; running on CPU.\n",
      "âœ” Opened camera index 0 via backend 200 at ~1280x720 @ 30.0 fps (warmup ok=20)\n",
      "Webcam released. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Robust WEBCAM realtime YOLO (multi-backend open + MJPG + warmup + threaded)\n",
    "# Hotkeys:\n",
    "#   q : quit\n",
    "#   - / = : lower/raise confidence\n",
    "#   [ / ] : smaller/larger inference imgsz\n",
    "#   n / m : increase/decrease inference stride\n",
    "#   a     : toggle augment\n",
    "#   r     : start/stop MP4 recording to runs/webcam/\n",
    "\n",
    "import os, time, threading, queue\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "MODEL_PATH        = \"runs/train/soccer_players_clean-exp/weights/best.pt\"\n",
    "PREFERRED_INDEXES = [0]   # try these camera indices in order\n",
    "TARGET_RES        = (1280, 720)    # request this capture size\n",
    "TARGET_FPS        = 30\n",
    "FORCE_MJPG        = True           # MJPG often boosts FPS on 720p/1080p\n",
    "WARMUP_FRAMES     = 20             # frames to grab before starting (verifies camera is alive)\n",
    "\n",
    "CONF              = 0.20\n",
    "IOU               = 0.60\n",
    "IMG_SIZE          = 960\n",
    "PROCESS_EVERY_N   = 3\n",
    "USE_AUGMENT       = False\n",
    "SHOW              = True\n",
    "# =================================================\n",
    "\n",
    "# Optional: avoid CPU oversubscription\n",
    "try: cv2.setNumThreads(0)\n",
    "except Exception: pass\n",
    "\n",
    "# Load model & device\n",
    "model = YOLO(MODEL_PATH)\n",
    "device_arg = 0\n",
    "try:\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "        device_arg = 'cpu'\n",
    "        print(\"CUDA not available; running on CPU.\")\n",
    "    else:\n",
    "        print(\"Using CUDA for inference.\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "except Exception:\n",
    "    device_arg = 'cpu'\n",
    "    print(\"Torch probe failed; running on CPU.\")\n",
    "\n",
    "# --- helpers ---\n",
    "def try_open_camera(index, backend):\n",
    "    cap = cv2.VideoCapture(index, backend)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    # Request format for speed\n",
    "    if FORCE_MJPG:\n",
    "        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH,  TARGET_RES[0])\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, TARGET_RES[1])\n",
    "    cap.set(cv2.CAP_PROP_FPS, TARGET_FPS)\n",
    "    try: cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "    except Exception: pass\n",
    "    return cap\n",
    "\n",
    "def open_any_camera(indexes):\n",
    "    # On Linux: prefer V4L2, then ANY\n",
    "    backends = []\n",
    "    if os.name != \"nt\":\n",
    "        backends = [cv2.CAP_V4L2, cv2.CAP_ANY]\n",
    "    else:\n",
    "        backends = [cv2.CAP_DSHOW, cv2.CAP_MSMF, cv2.CAP_ANY]\n",
    "\n",
    "    for idx in indexes:\n",
    "        for be in backends:\n",
    "            cap = try_open_camera(idx, be)\n",
    "            if cap is None:\n",
    "                continue\n",
    "            # Warmup: try to read WARMUP_FRAMES inside 2 seconds\n",
    "            ok_count = 0\n",
    "            t0 = time.time()\n",
    "            for _ in range(WARMUP_FRAMES):\n",
    "                ok, _ = cap.read()\n",
    "                if ok: ok_count += 1\n",
    "                if time.time() - t0 > 2.0:\n",
    "                    break\n",
    "            if ok_count >= max(5, WARMUP_FRAMES // 3):\n",
    "                w  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or TARGET_RES[0])\n",
    "                h  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or TARGET_RES[1])\n",
    "                fps = cap.get(cv2.CAP_PROP_FPS) or TARGET_FPS\n",
    "                print(f\"âœ” Opened camera index {idx} via backend {be} at ~{w}x{h} @ {fps:.1f} fps (warmup ok={ok_count})\")\n",
    "                return cap, idx, be\n",
    "            else:\n",
    "                print(f\"âš  Camera {idx} backend {be} produced no/slow frames in warmup (ok={ok_count}); trying nextâ€¦\")\n",
    "                cap.release()\n",
    "    return None, None, None\n",
    "\n",
    "cap, cam_idx, backend = open_any_camera(PREFERRED_INDEXES)\n",
    "if cap is None:\n",
    "    raise RuntimeError(\"âŒ Could not open any webcam. Make sure no other app is using it and try a different index.\")\n",
    "\n",
    "# ---------- Threaded reader: keep only latest frame ----------\n",
    "frame_q: \"queue.Queue[tuple[float, any]]\" = queue.Queue(maxsize=1)\n",
    "stop_flag = False\n",
    "\n",
    "def reader_loop():\n",
    "    global stop_flag\n",
    "    while not stop_flag:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            time.sleep(0.005)\n",
    "            continue\n",
    "        ts = time.time()\n",
    "        # drop stale frame (maxsize=1)\n",
    "        if frame_q.full():\n",
    "            try: frame_q.get_nowait()\n",
    "            except Exception: pass\n",
    "        frame_q.put((ts, frame))\n",
    "    stop_flag = True\n",
    "\n",
    "t_reader = threading.Thread(target=reader_loop, daemon=True)\n",
    "t_reader.start()\n",
    "\n",
    "# Optional MP4 recorder\n",
    "save_root = Path(\"runs/webcam\"); save_root.mkdir(parents=True, exist_ok=True)\n",
    "writer, recording = None, False\n",
    "def start_writer(frame):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    outp = save_root / f\"webcam_{int(time.time())}.mp4\"\n",
    "    h, w = frame.shape[:2]\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    vw = cv2.VideoWriter(str(outp), fourcc, fps, (w, h))\n",
    "    return vw, outp\n",
    "\n",
    "# Main loop\n",
    "last_annot = None\n",
    "shown = 0\n",
    "t0 = time.time()\n",
    "frame_idx = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        try:\n",
    "            ts, frame = frame_q.get(timeout=2.0)\n",
    "        except queue.Empty:\n",
    "            print(\"âš  No frames from camera; still waitingâ€¦ (is it in use by another app?)\")\n",
    "            continue\n",
    "\n",
    "        run_infer = (frame_idx % max(1, int(PROCESS_EVERY_N)) == 0) or (last_annot is None)\n",
    "\n",
    "        if run_infer:\n",
    "            # Version-safe inference\n",
    "            try:\n",
    "                results = model(frame,\n",
    "                                conf=CONF, iou=IOU, imgsz=IMG_SIZE,\n",
    "                                augment=USE_AUGMENT, agnostic_nms=True, max_det=120,\n",
    "                                device=device_arg, verbose=False)\n",
    "            except TypeError:\n",
    "                results = model(frame,\n",
    "                                conf=CONF, iou=IOU, imgsz=IMG_SIZE,\n",
    "                                agnostic_nms=True, max_det=120,\n",
    "                                device=device_arg, verbose=False)\n",
    "            last_annot = results[0].plot()\n",
    "\n",
    "        # HUD (display FPS)\n",
    "        shown += 1\n",
    "        if shown % 10 == 0:\n",
    "            fps_disp = shown / (time.time() - t0 + 1e-9)\n",
    "            hud = f\"Disp~{fps_disp:.1f}  conf:{CONF:.2f}  imgsz:{IMG_SIZE}  N:{PROCESS_EVERY_N}  aug:{USE_AUGMENT}\"\n",
    "            cv2.putText(last_annot, hud, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if SHOW:\n",
    "            # Create window once and make sure it stays on top (optional)\n",
    "            # cv2.namedWindow(\"YOLOv8 Webcam\", cv2.WINDOW_NORMAL)\n",
    "            cv2.imshow(\"YOLOv8 Webcam (q quit, -/= conf, [/] size, n/m stride, a aug, r rec)\", last_annot)\n",
    "\n",
    "        # Recording\n",
    "        if recording:\n",
    "            if writer is None:\n",
    "                writer, outfile = start_writer(last_annot)\n",
    "                print(\"âº Recording to:\", outfile)\n",
    "            writer.write(last_annot)\n",
    "        elif writer is not None:\n",
    "            writer.release(); writer = None\n",
    "            print(\"â¹ Recording stopped.\")\n",
    "\n",
    "        # Hotkeys\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "        elif k == ord('-'):\n",
    "            CONF = max(0.01, CONF - 0.05)\n",
    "        elif k in (ord('='), ord('+')):\n",
    "            CONF = min(0.99, CONF + 0.05)\n",
    "        elif k == ord('['):\n",
    "            IMG_SIZE = max(320, IMG_SIZE - 64)\n",
    "        elif k == ord(']'):\n",
    "            IMG_SIZE = min(1536, IMG_SIZE + 64)\n",
    "        elif k == ord('a'):\n",
    "            USE_AUGMENT = not USE_AUGMENT; print(\"augment =\", USE_AUGMENT)\n",
    "        elif k == ord('n'):\n",
    "            PROCESS_EVERY_N = max(1, PROCESS_EVERY_N + 1); print(\"process every N frames:\", PROCESS_EVERY_N)\n",
    "        elif k == ord('m'):\n",
    "            PROCESS_EVERY_N = max(1, PROCESS_EVERY_N - 1); print(\"process every N frames:\", PROCESS_EVERY_N)\n",
    "        elif k == ord('r'):\n",
    "            recording = not recording\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    try:\n",
    "        stop_flag = True\n",
    "        t_reader.join(timeout=1.0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Webcam released. Goodbye!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afdde89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Trying to install onnxsim (optional)â€¦\n",
      "onnxsim not available â€” simplification will be skipped.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install --upgrade pip setuptools wheel\n",
    "\n",
    "# Avoid OpenCV/NumPy conflicts (keep GUI build of OpenCV + NumPy 1.26.x)\n",
    "%pip -q uninstall -y opencv-python-headless opencv-contrib-python-headless opencv-contrib-python || true\n",
    "%pip -q install \"opencv-python>=4.9,<4.13\" \"numpy==1.26.4\"\n",
    "\n",
    "# Core libs with Py3.12 wheels\n",
    "%pip -q install \"ultralytics==8.2.0\" \\\n",
    "                \"onnx>=1.16.1,<1.18\" \\\n",
    "                \"onnxruntime>=1.17.0,<1.19\" \\\n",
    "                \"pyyaml\" \"tqdm\" \"matplotlib\" \"pandas\"\n",
    "\n",
    "# Try ONNX simplifier (optional). If no wheel for your platform, we'll skip simplification later.\n",
    "try:\n",
    "    import onnxsim  # noqa\n",
    "    print(\"onnxsim already installed\")\n",
    "except Exception:\n",
    "    print(\"Trying to install onnxsim (optional)â€¦\")\n",
    "    rc = !pip install -q --only-binary=:all: onnxsim\n",
    "    try:\n",
    "        import onnxsim  # noqa\n",
    "        print(\"onnxsim installed\")\n",
    "    except Exception:\n",
    "        print(\"onnxsim not available â€” simplification will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccebe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2 module: <module 'cv2' from '/home/surya/c/.venv/lib/python3.12/site-packages/cv2/__init__.py'>\n",
      "cv2.__file__: /home/surya/c/.venv/lib/python3.12/site-packages/cv2/__init__.py\n",
      "Has setNumThreads? True\n",
      "Local name collisions: none\n",
      "cv2.setNumThreads is available\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import cv2\n",
    "\n",
    "# Diagnose what got imported\n",
    "print(\"cv2 module:\", cv2)\n",
    "print(\"cv2.__file__:\", getattr(cv2, \"__file__\", \"None\"))\n",
    "print(\"Has setNumThreads?\", hasattr(cv2, \"setNumThreads\"))\n",
    "\n",
    "# Common local collisions (rename/delete if found, then restart kernel)\n",
    "collisions = []\n",
    "for name in (\"cv2.py\", \"cv2\", \"opencv.py\", \"opencv\"):\n",
    "    p = os.path.join(os.getcwd(), name)\n",
    "    if os.path.exists(p):\n",
    "        collisions.append(p)\n",
    "print(\"Local name collisions:\", collisions or \"none\")\n",
    "\n",
    "# ---- Compatibility shim: add a no-op setNumThreads if missing ----\n",
    "if not hasattr(cv2, \"setNumThreads\"):\n",
    "    def _noop(*args, **kwargs):  # harmless no-op\n",
    "        return None\n",
    "    cv2.setNumThreads = _noop  # type: ignore\n",
    "    print(\"Shimmed cv2.setNumThreads â†’ no-op\")\n",
    "else:\n",
    "    print(\"cv2.setNumThreads is available\")\n",
    "\n",
    "# (Optional) Also ensure getNumThreads exists to avoid any other surprises\n",
    "if not hasattr(cv2, \"getNumThreads\"):\n",
    "    cv2.getNumThreads = lambda: 0  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83145a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx: 1.17.0 | onnxruntime: 1.18.1 | providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import os, time, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx, onnxruntime as ort\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"onnx:\", onnx.__version__, \"| onnxruntime:\", ort.__version__, \"| providers:\", ort.get_available_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b404591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights: runs/train/soccer_players_clean-exp/weights/best.pt\n",
      "Export dir: /home/surya/c/hailo_build/exports\n"
     ]
    }
   ],
   "source": [
    "# Your trained weights\n",
    "MODEL_PT   = Path(\"runs/train/soccer_players_clean-exp/weights/best.pt\")\n",
    "\n",
    "# Export settings (Hailo-friendly)\n",
    "IMG_SIZE   = 960         # choose 640 / 960 / 1280; keep consistent later\n",
    "OPSET      = 12          # 11â€“13 fine; 12 is safe\n",
    "INCLUDE_NMS= False       # Hailo expects raw heads\n",
    "DYNAMIC    = False       # static [1,3,IMG,IMG] preferred by compilers\n",
    "\n",
    "# Output dir\n",
    "EXPORTS_DIR = Path(\"hailo_build/exports\")\n",
    "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert MODEL_PT.exists(), f\"Missing weights: {MODEL_PT}\"\n",
    "print(\"Using weights:\", MODEL_PT)\n",
    "print(\"Export dir:\", EXPORTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b1364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting torch<2.6\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.5.1%2Bcpu-cp312-cp312-linux_x86_64.whl (174.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.6/174.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch<2.6) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch<2.6) (4.14.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch<2.6) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch<2.6) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch<2.6) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch<2.6) (80.9.0)\n",
      "Collecting sympy==1.13.1 (from torch<2.6)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch<2.6) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch<2.6) (3.0.2)\n",
      "Installing collected packages: sympy, torch\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: torchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: torch 2.8.0\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling torch-2.8.0:[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.8.0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnxslim 0.1.65 requires sympy>=1.13.3, but you have sympy 1.13.1 which is incompatible.\n",
      "torchvision 0.23.0 requires torch==2.8.0, but you have torch 2.5.1+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sympy-1.13.1 torch-2.5.1+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch<2.6\" --extra-index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched torch.load â‡’ default weights_only=False\n",
      "Added safe globals for Ultralytics + torch.nn containers\n"
     ]
    }
   ],
   "source": [
    "# --- Patch cv2 & torch.load for this session ---\n",
    "\n",
    "# 1) Some OpenCV wheels lack setNumThreads; Ultralytics calls it on import.\n",
    "import cv2\n",
    "if not hasattr(cv2, \"setNumThreads\"):\n",
    "    cv2.setNumThreads = lambda *a, **k: None  # harmless no-op\n",
    "    print(\"Shimmed cv2.setNumThreads â†’ no-op\")\n",
    "\n",
    "# 2) PyTorch 2.6 defaults weights_only=True; force False so Ultralytics checkpoints load.\n",
    "import torch\n",
    "_orig_torch_load = torch.load\n",
    "def _torch_load_patched(*args, **kwargs):\n",
    "    if \"weights_only\" not in kwargs:\n",
    "        kwargs[\"weights_only\"] = False  # allow class pickles (you trust this file)\n",
    "    return _orig_torch_load(*args, **kwargs)\n",
    "torch.load = _torch_load_patched\n",
    "print(\"Patched torch.load â‡’ default weights_only=False\")\n",
    "\n",
    "# 3) (Optional) also allowlist common classes to reduce future prompts\n",
    "try:\n",
    "    from torch.serialization import add_safe_globals\n",
    "    from ultralytics.nn.tasks import DetectionModel, SegmentationModel, ClassificationModel\n",
    "    from torch.nn.modules.container import Sequential, ModuleList\n",
    "    add_safe_globals([DetectionModel, SegmentationModel, ClassificationModel, Sequential, ModuleList])\n",
    "    print(\"Added safe globals for Ultralytics + torch.nn containers\")\n",
    "except Exception as e:\n",
    "    print(\"Safe-globals add skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec23c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched torch.load â‡’ default weights_only=False\n",
      "Added safe globals for Ultralytics + torch.nn containers\n",
      "Loading weights: runs/train/soccer_players_clean-exp2/weights/best.pt\n",
      "Exporting ONNXâ€¦\n",
      "Ultralytics YOLOv8.2.0 ðŸš€ Python-3.12.3 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i5-11300H 3.10GHz)\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/train/soccer_players_clean-exp2/weights/best.pt' with input shape (1, 3, 960, 960) BCHW and output shape(s) (1, 7, 18900) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.8s, saved as 'runs/train/soccer_players_clean-exp2/weights/best.onnx' (11.9 MB)\n",
      "\n",
      "Export complete (2.9s)\n",
      "Results saved to \u001b[1m/home/surya/c/runs/train/soccer_players_clean-exp2/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs/train/soccer_players_clean-exp2/weights/best.onnx imgsz=960  \n",
      "Validate:        yolo val task=detect model=runs/train/soccer_players_clean-exp2/weights/best.onnx imgsz=960 data=/home/surya/c/datasets/soccer players.v1i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "âœ… ONNX exported to: hailo_build/exports/best.onnx\n",
      "\n",
      "Inputs:\n",
      " - images [1, 3, 960, 960]\n",
      "Outputs:\n",
      " - output0 [1, 7, 18900]\n",
      "\n",
      "Summary â†’ input: images | outputs: ['output0']\n",
      "\n",
      "All done. You can now proceed with Hailo Model Compiler.\n"
     ]
    }
   ],
   "source": [
    "# === Robust ONNX export (fixes Torch 2.6 unpickler + missing DFLoss + cv2 threads) ===\n",
    "import os, time, shutil, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- EDIT if your run path differs ----\n",
    "MODEL_PT    = Path(\"runs/train/soccer_players_clean-exp2/weights/best.pt\")\n",
    "IMG_SIZE    = 960\n",
    "OPSET       = 12\n",
    "INCLUDE_NMS = False  # Hailo expects raw heads\n",
    "DYNAMIC     = False  # static [1,3,IMG,IMG] for hardware compilers\n",
    "EXPORTS_DIR = Path(\"hailo_build/exports\"); EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# ---------------------------------------\n",
    "\n",
    "# 0) OpenCV shim: Ultralytics calls cv2.setNumThreads on import\n",
    "import cv2\n",
    "if not hasattr(cv2, \"setNumThreads\"):\n",
    "    cv2.setNumThreads = lambda *a, **k: None  # harmless no-op\n",
    "    print(\"Shimmed cv2.setNumThreads â†’ no-op\")\n",
    "\n",
    "# 1) PyTorch 2.6: force weights_only=False for this session (you trust your file)\n",
    "import torch\n",
    "if not hasattr(torch, \"_orig_load\"):\n",
    "    torch._orig_load = torch.load\n",
    "def _torch_load_patched(*args, **kwargs):\n",
    "    if \"weights_only\" not in kwargs:\n",
    "        kwargs[\"weights_only\"] = False\n",
    "    return torch._orig_load(*args, **kwargs)\n",
    "torch.load = _torch_load_patched\n",
    "print(\"Patched torch.load â‡’ default weights_only=False\")\n",
    "\n",
    "# 2) Pre-import Ultralytics & inject DFLoss stub if missing\n",
    "#    (not used at inference/export; just satisfies unpickler for older checkpoints)\n",
    "try:\n",
    "    from ultralytics.utils import loss as uloss\n",
    "    import torch.nn as nn\n",
    "    if not hasattr(uloss, \"DFLoss\"):\n",
    "        class DFLoss(nn.Module):\n",
    "            def __init__(self, *a, **k): super().__init__()\n",
    "            def forward(self, *a, **k):  # should never be called during export/inference\n",
    "                raise RuntimeError(\"DFLoss stub was called unexpectedly.\")\n",
    "        uloss.DFLoss = DFLoss\n",
    "        print(\"Injected stub ultralytics.utils.loss.DFLoss\")\n",
    "except Exception as e:\n",
    "    print(\"Note: could not import ultralytics.utils.loss yet:\", e)\n",
    "\n",
    "# 3) Allowlist common classes to reduce further prompts (optional)\n",
    "try:\n",
    "    from torch.serialization import add_safe_globals\n",
    "    from torch.nn.modules.container import Sequential, ModuleList\n",
    "    from ultralytics.nn.tasks import DetectionModel, SegmentationModel, ClassificationModel\n",
    "    allow = [DetectionModel, SegmentationModel, ClassificationModel, Sequential, ModuleList]\n",
    "    try:\n",
    "        from ultralytics.nn.tasks import PoseModel\n",
    "        allow.append(PoseModel)\n",
    "    except Exception:\n",
    "        pass\n",
    "    add_safe_globals(allow)\n",
    "    print(\"Added safe globals for Ultralytics + torch.nn containers\")\n",
    "except Exception as e:\n",
    "    print(\"Safe-globals add skipped:\", e)\n",
    "\n",
    "# 4) Load YOLO and export ONNX\n",
    "from ultralytics import YOLO\n",
    "assert MODEL_PT.exists(), f\"Missing weights: {MODEL_PT}\"\n",
    "print(\"Loading weights:\", MODEL_PT)\n",
    "model = YOLO(str(MODEL_PT))\n",
    "\n",
    "print(\"Exporting ONNXâ€¦\")\n",
    "onnx_path = model.export(\n",
    "    format   = \"onnx\",\n",
    "    imgsz    = IMG_SIZE,\n",
    "    opset    = OPSET,\n",
    "    nms      = INCLUDE_NMS,   # keep False for Hailo\n",
    "    dynamic  = DYNAMIC,       # keep static shape\n",
    "    simplify = False          # simplifier optional; skip for compatibility\n",
    ")\n",
    "\n",
    "# 5) Persist into our exports dir\n",
    "onnx_path = Path(onnx_path)\n",
    "onnx_out  = EXPORTS_DIR / f\"{onnx_path.stem}.onnx\"\n",
    "if onnx_path.resolve() != onnx_out.resolve():\n",
    "    shutil.copy2(onnx_path, onnx_out)\n",
    "print(\"âœ… ONNX exported to:\", onnx_out)\n",
    "\n",
    "# 6) Verify structure & print I/O names (needed for Hailo YAML)\n",
    "import onnx\n",
    "m = onnx.load(str(onnx_out))\n",
    "onnx.checker.check_model(m)\n",
    "def shp(v):  # pretty shape\n",
    "    return [d.dim_value if d.HasField(\"dim_value\") else \"?\" for d in v.type.tensor_type.shape.dim]\n",
    "inputs  = list(m.graph.input)\n",
    "outputs = list(m.graph.output)\n",
    "print(\"\\nInputs:\")\n",
    "for i in inputs:  print(\" -\", i.name, shp(i))\n",
    "print(\"Outputs:\")\n",
    "for o in outputs: print(\" -\", o.name, shp(o))\n",
    "ONNX_INPUT_NAME = inputs[0].name\n",
    "ONNX_OUTPUTS    = [o.name for o in outputs]\n",
    "print(\"\\nSummary â†’ input:\", ONNX_INPUT_NAME, \"| outputs:\", ONNX_OUTPUTS)\n",
    "print(\"\\nAll done. You can now proceed with Hailo Model Compiler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fba84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ONNX: hailo_build/exports/best.onnx\n",
      "Inputs:\n",
      " - images [1, 3, 960, 960]\n",
      "Outputs:\n",
      " - output0 [1, 7, 18900]\n",
      "Classes: ['0', '1', '2']\n",
      "num_classes: 3\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, yaml, time, shutil, random\n",
    "import onnx\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==== EDIT THESE IF NEEDED ====\n",
    "EXPORTS_DIR  = Path(\"hailo_build/exports\")\n",
    "ONNX_CANDID  = sorted(EXPORTS_DIR.glob(\"*.onnx\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert ONNX_CANDID, \"No ONNX found. Run the ONNX export cells first.\"\n",
    "ONNX_PATH    = ONNX_CANDID[0]   # latest\n",
    "\n",
    "DATASET_ROOT = Path(\"/home/surya/Downloads/soccer players.v1i.yolov8\")  # your dataset folder\n",
    "RUN_DIR      = Path(\"runs/train/soccer_players_clean-exp\")\n",
    "MODEL_PT     = RUN_DIR / \"weights/best.pt\"\n",
    "IMG_SIZE     = 960              # must match your ONNX export size\n",
    "CALIB_MAX    = 800              # number of calibration images to collect\n",
    "\n",
    "WORK_DIR     = Path(\"hailo_build\")\n",
    "CALIB_DIR    = WORK_DIR / \"calib\"\n",
    "DFC_YAML     = WORK_DIR / \"model.yaml\"\n",
    "HEF_DIR      = WORK_DIR / \"hef\"\n",
    "for p in (CALIB_DIR, HEF_DIR): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using ONNX:\", ONNX_PATH)\n",
    "\n",
    "# Parse ONNX I/O names (needed for DFC YAML)\n",
    "m = onnx.load(str(ONNX_PATH))\n",
    "inputs  = list(m.graph.input)\n",
    "outputs = list(m.graph.output)\n",
    "def shp(v): return [d.dim_value if d.HasField(\"dim_value\") else \"?\" for d in v.type.tensor_type.shape.dim]\n",
    "print(\"Inputs:\")\n",
    "for i in inputs:  print(\" -\", i.name, shp(i))\n",
    "print(\"Outputs:\")\n",
    "for o in outputs: print(\" -\", o.name, shp(o))\n",
    "\n",
    "ONNX_INPUT_NAME = inputs[0].name\n",
    "ONNX_OUTPUTS    = [o.name for o in outputs]\n",
    "\n",
    "# Find class names\n",
    "def find_data_yaml(root: Path):\n",
    "    if (root / \"data.yaml\").exists(): return root / \"data.yaml\"\n",
    "    for q in root.rglob(\"data.yaml\"):\n",
    "        return q\n",
    "    if (RUN_DIR / \"data.yaml\").exists(): return RUN_DIR / \"data.yaml\"\n",
    "    for q in RUN_DIR.rglob(\"data.yaml\"):\n",
    "        return q\n",
    "    return None\n",
    "\n",
    "CLASS_NAMES, NUM_CLASSES = [], 80\n",
    "dy = find_data_yaml(DATASET_ROOT)\n",
    "if dy and dy.exists():\n",
    "    spec = yaml.safe_load(dy.read_text())\n",
    "    nm = spec.get(\"names\", [])\n",
    "    if isinstance(nm, dict): CLASS_NAMES = [nm[i] for i in sorted(map(int, nm.keys()))]\n",
    "    elif isinstance(nm, list): CLASS_NAMES = nm\n",
    "    if CLASS_NAMES: NUM_CLASSES = len(CLASS_NAMES)\n",
    "elif MODEL_PT.exists():\n",
    "    y = YOLO(str(MODEL_PT))\n",
    "    names = y.names if isinstance(y.names, dict) else {i:n for i,n in enumerate(y.names)}\n",
    "    CLASS_NAMES = [names[i] for i in sorted(names.keys())]\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(\"Classes:\", CLASS_NAMES if CLASS_NAMES else \"(unknown)\")\n",
    "print(\"num_classes:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031e2347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying calibration images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [00:00<00:00, 2179.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calibration images prepared: 163 at /home/surya/c/hailo_build/calib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "CALIB_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
    "CALIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "candidates = []\n",
    "for p in DATASET_ROOT.rglob(\"*\"):\n",
    "    if p.is_file() and p.suffix.lower() in CALIB_EXTS:\n",
    "        candidates.append(p)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(candidates)\n",
    "pick = candidates[:CALIB_MAX] if len(candidates) > CALIB_MAX else candidates\n",
    "\n",
    "copied = 0\n",
    "for src in tqdm(pick, desc=\"Copying calibration images\"):\n",
    "    try:\n",
    "        dst = CALIB_DIR / src.name\n",
    "        if not dst.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "        copied += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"âœ… Calibration images prepared: {copied} at {CALIB_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cdc9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2, time\n",
    "from pathlib import Path\n",
    "\n",
    "# Keep consistent with export\n",
    "IMG_SIZE = 960\n",
    "CLASS_NAMES = globals().get(\"CLASS_NAMES\", [])  # from earlier; else set manually: [\"player\",\"ball\",\"referee\",...]\n",
    "CONF_THR = 0.25\n",
    "IOU_THR  = 0.60\n",
    "MAX_DET  = 300\n",
    "\n",
    "def letterbox(im, new_size=640, color=(114,114,114), stride=32):\n",
    "    \"\"\"Resize + pad to square while preserving aspect ratio; returns image, ratio, padding.\"\"\"\n",
    "    shape = im.shape[:2]  # (h,w)\n",
    "    if isinstance(new_size, int):\n",
    "        new_size = (new_size, new_size)\n",
    "    r = min(new_size[0] / shape[0], new_size[1] / shape[1])\n",
    "    new_unpad = (int(round(shape[1] * r)), int(round(shape[0] * r)))\n",
    "    dw, dh = new_size[1] - new_unpad[0], new_size[0] - new_unpad[1]\n",
    "    dw /= 2; dh /= 2\n",
    "    if shape[::-1] != new_unpad:\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh-0.1)), int(round(dh+0.1))\n",
    "    left, right = int(round(dw-0.1)), int(round(dw+0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return im, r, (left, top)\n",
    "\n",
    "def nms_numpy(boxes, scores, iou_thr=0.5, max_det=300):\n",
    "    \"\"\"Classic NMS in NumPy on xyxy boxes.\"\"\"\n",
    "    if boxes.size == 0:\n",
    "        return []\n",
    "    x1, y1, x2, y2 = boxes.T\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0 and len(keep) < max_det:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        inter = w * h\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n",
    "        inds = np.where(iou <= iou_thr)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "def draw_dets(frame, dets):\n",
    "    \"\"\"dets: list of (xyxy, conf, cls_id).\"\"\"\n",
    "    for (x1,y1,x2,y2), conf, cid in dets:\n",
    "        name = CLASS_NAMES[cid] if CLASS_NAMES and cid < len(CLASS_NAMES) else str(cid)\n",
    "        cv2.rectangle(frame, (int(x1),int(y1)), (int(x2),int(y2)), (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"{name} {conf:.2f}\", (int(x1), max(0,int(y1)-5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "013ddd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ONNX: hailo_build/exports/best.onnx\n",
      "Input: images shape: [1, 3, 960, 960]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Prefer CUDA if available\n",
    "providers = [\"CPUExecutionProvider\"]\n",
    "try:\n",
    "    if \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
    "        providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "        print(\"Using ORT CUDAExecutionProvider\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Find the latest ONNX exported earlier\n",
    "EXPORTS_DIR = Path(\"hailo_build/exports\")\n",
    "onnx_list = sorted(EXPORTS_DIR.glob(\"*.onnx\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert onnx_list, \"No ONNX in hailo_build/exports\"\n",
    "ONNX_PATH = onnx_list[0]\n",
    "print(\"Using ONNX:\", ONNX_PATH)\n",
    "\n",
    "sess = ort.InferenceSession(str(ONNX_PATH), providers=providers)\n",
    "inp = sess.get_inputs()[0]\n",
    "inp_name = inp.name\n",
    "print(\"Input:\", inp_name, \"shape:\", [d if isinstance(d,int) else '?' for d in inp.shape])\n",
    "\n",
    "def preprocess(frame_bgr):\n",
    "    img, r, (padw, padh) = letterbox(frame_bgr, new_size=IMG_SIZE)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    x = img_rgb.astype(np.float32) / 255.0\n",
    "    x = np.transpose(x, (2,0,1))[None, ...]  # [1,3,H,W]\n",
    "    return x, r, padw, padh, img.shape[1], img.shape[0]\n",
    "\n",
    "def postprocess(ort_outs, r, padw, padh, orig_w, orig_h, conf_thr=CONF_THR, iou_thr=IOU_THR):\n",
    "    \"\"\"\n",
    "    Assumes Ultralytics-style output: [1, N, 5+C] with columns [cx,cy,w,h, obj, class_probs...]\n",
    "    If your export produced a list, we use the first with rank 3.\n",
    "    \"\"\"\n",
    "    y = None\n",
    "    if isinstance(ort_outs, (list,tuple)):\n",
    "        for t in ort_outs:\n",
    "            if isinstance(t, np.ndarray) and t.ndim == 3 and t.shape[0] == 1:\n",
    "                y = t\n",
    "                break\n",
    "    elif isinstance(ort_outs, np.ndarray):\n",
    "        y = ort_outs\n",
    "    assert y is not None, f\"Unexpected ONNX outputs: {[o.shape if hasattr(o,'shape') else type(o) for o in (ort_outs if isinstance(ort_outs,(list,tuple)) else [ort_outs])]}\"\n",
    "\n",
    "    preds = y[0]  # [N, 5+C]\n",
    "    if preds.shape[1] < 6:\n",
    "        return []\n",
    "\n",
    "    b = preds[:, :4]             # cx,cy,w,h (on padded/letterboxed canvas IMG_SIZE)\n",
    "    obj = preds[:, 4:5]          # [N,1]\n",
    "    cls = preds[:, 5:]           # [N,C]\n",
    "    scores = obj * cls           # [N,C]\n",
    "    cls_ids = scores.argmax(axis=1)\n",
    "    confs = scores.max(axis=1)\n",
    "\n",
    "    # threshold\n",
    "    m = confs >= conf_thr\n",
    "    if not np.any(m):\n",
    "        return []\n",
    "    b = b[m]; confs = confs[m]; cls_ids = cls_ids[m]\n",
    "\n",
    "    # xywh â†’ xyxy on letterboxed canvas\n",
    "    cx, cy, w, h = b[:,0], b[:,1], b[:,2], b[:,3]\n",
    "    x1 = cx - w/2; y1 = cy - h/2; x2 = cx + w/2; y2 = cy + h/2\n",
    "    boxes = np.stack([x1, y1, x2, y2], axis=1)\n",
    "\n",
    "    # map back from letterbox canvas (IMG_SIZE) to original frame\n",
    "    # undo padding, then divide by r\n",
    "    boxes[:, [0,2]] -= padw\n",
    "    boxes[:, [1,3]] -= padh\n",
    "    boxes /= r\n",
    "\n",
    "    # clip\n",
    "    boxes[:, [0,2]] = boxes[:, [0,2]].clip(0, orig_w-1)\n",
    "    boxes[:, [1,3]] = boxes[:, [1,3]].clip(0, orig_h-1)\n",
    "\n",
    "    # NMS per-class (simple way: single-class NMS on scores; or do per-class loop)\n",
    "    keep = nms_numpy(boxes, confs, iou_thr=iou_thr, max_det=MAX_DET)\n",
    "    boxes = boxes[keep]; confs = confs[keep]; cls_ids = cls_ids[keep]\n",
    "\n",
    "    return [ (boxes[i], float(confs[i]), int(cls_ids[i])) for i in range(len(keep)) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1772da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open webcam\n",
    "SRC_INDEX = 0  # change if you have multiple cams\n",
    "cap = cv2.VideoCapture(SRC_INDEX)\n",
    "assert cap.isOpened(), \"Could not open webcam\"\n",
    "\n",
    "# ask for 1280x720; driver may adjust\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "shown, t0 = 0, time.time()\n",
    "try:\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        x, r, padw, padh, Wc, Hc = preprocess(frame)\n",
    "        outs = sess.run(None, {inp_name: x})\n",
    "        dets = postprocess(outs, r, padw, padh, frame.shape[1], frame.shape[0], CONF_THR, IOU_THR)\n",
    "        out = draw_dets(frame.copy(), dets)\n",
    "\n",
    "        shown += 1\n",
    "        if shown % 10 == 0:\n",
    "            fps = shown / (time.time() - t0 + 1e-9)\n",
    "            cv2.putText(out, f\"FPS~{fps:.1f} conf:{CONF_THR:.2f} iou:{IOU_THR:.2f} imgsz:{IMG_SIZE}\",\n",
    "                        (10,28), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"ONNX YOLOv8 â€” Webcam (q quit, -/= conf, [/] size)\", out)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == ord('q'): break\n",
    "        elif k == ord('-'): CONF_THR = max(0.01, CONF_THR - 0.05)\n",
    "        elif k in (ord('='), ord('+')): CONF_THR = min(0.99, CONF_THR + 0.05)\n",
    "        elif k == ord('['): IMG_SIZE = max(320, IMG_SIZE - 64)\n",
    "        elif k == ord(']'): IMG_SIZE = min(1536, IMG_SIZE + 64)\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c68dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9efbef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MP4: downloads/FC Dallas vs. LAFC ï½œ Full Match Highlights ï½œ Son Heung-Min BANGER!!.mp4\n",
      "âœ… First frame OK: 1920x1080 @ ~59.9 FPS\n",
      "First frame saved: downloads/first_frame_debug.jpg\n"
     ]
    }
   ],
   "source": [
    "import os, stat, time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from yt_dlp import YoutubeDL\n",
    "import imageio_ffmpeg\n",
    "import numpy as np\n",
    "\n",
    "# --- sanity checks on prerequisites (defined in earlier cells) ---\n",
    "for name in [\"sess\",\"inp_name\",\"preprocess\",\"postprocess\",\"draw_dets\",\"IMG_SIZE\",\"CONF_THR\",\"IOU_THR\"]:\n",
    "    assert name in globals(), f\"Missing {name}. Run the earlier ONNX cells first.\"\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=-D5lO5Pl-3Q\"  # replace if needed\n",
    "TARGET_H  = 720  # 480/720 is easier to decode fast on CPU\n",
    "\n",
    "# Ensure ffmpeg is available on PATH\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "try: os.chmod(ffmpeg_path, os.stat(ffmpeg_path).st_mode | stat.S_IEXEC)\n",
    "except Exception: pass\n",
    "os.environ[\"PATH\"] = os.path.dirname(ffmpeg_path) + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "\n",
    "def download_mp4_h264(url: str, max_h=720) -> Path:\n",
    "    \"\"\"\n",
    "    Always download a progressive mp4 (H.264 video + m4a audio).\n",
    "    This avoids brittle live-stream quirks and AV1 decode issues.\n",
    "    \"\"\"\n",
    "    out_dir = Path(\"downloads\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        \"outtmpl\": str(out_dir / \"%(title).90s.%(ext)s\"),\n",
    "        \"format\": (\n",
    "            f\"bestvideo[ext=mp4][vcodec^=avc1][height<={max_h}]+bestaudio[ext=m4a]/\"\n",
    "            f\"best[ext=mp4][vcodec^=avc1][height<={max_h}]/best\"\n",
    "        ),\n",
    "        \"merge_output_format\": \"mp4\",\n",
    "        \"ffmpeg_location\": os.path.dirname(ffmpeg_path),\n",
    "        \"noplaylist\": True,\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        fn = Path(ydl.prepare_filename(info)).with_suffix(\".mp4\")\n",
    "    assert fn.exists(), \"yt-dlp did not produce an MP4\"\n",
    "    return fn\n",
    "\n",
    "def open_and_verify_cap(video_path: Path) -> cv2.VideoCapture:\n",
    "    \"\"\"\n",
    "    Open the file and make sure we can read at least one frame.\n",
    "    Also prints resolution/FPS, and dumps the first frame to disk for debugging.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path), cv2.CAP_FFMPEG)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Could not open: {video_path}\")\n",
    "\n",
    "    # Try to read a first frame (with small retry loop)\n",
    "    ok, frame = cap.read()\n",
    "    tries = 0\n",
    "    while (not ok or frame is None) and tries < 50:\n",
    "        ok, frame = cap.read()\n",
    "        tries += 1\n",
    "\n",
    "    if not ok or frame is None:\n",
    "        cap.release()\n",
    "        raise RuntimeError(\"Video opened but returned no frames (codec issue?).\")\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    print(f\"âœ… First frame OK: {w}x{h} @ ~{fps:.1f} FPS\")\n",
    "    dbg = Path(\"downloads/first_frame_debug.jpg\")\n",
    "    cv2.imwrite(str(dbg), frame)\n",
    "    print(\"First frame saved:\", dbg)\n",
    "\n",
    "    # Reset to the first frame for playback\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    return cap\n",
    "\n",
    "# 1) Download progressive MP4\n",
    "mp4 = download_mp4_h264(VIDEO_URL, max_h=TARGET_H)\n",
    "print(\"Using MP4:\", mp4)\n",
    "\n",
    "# 2) Open and verify frames\n",
    "cap = open_and_verify_cap(mp4)\n",
    "\n",
    "# 3) Play with inference\n",
    "cv2.namedWindow(\"ONNX YOLOv8 â€” Video (q quit, -/= conf, [/] size)\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"ONNX YOLOv8 â€” Video (q quit, -/= conf, [/] size)\", 960, 540)\n",
    "\n",
    "t0, shown = time.time(), 0\n",
    "try:\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        x, r, padw, padh, Wc, Hc = preprocess(frame)\n",
    "        outs = sess.run(None, {inp_name: x})\n",
    "        dets = postprocess(outs, r, padw, padh, frame.shape[1], frame.shape[0], CONF_THR, IOU_THR)\n",
    "        out  = draw_dets(frame.copy(), dets)\n",
    "\n",
    "        shown += 1\n",
    "        if shown % 10 == 0:\n",
    "            fps = shown / (time.time() - t0 + 1e-9)\n",
    "            cv2.putText(out, f\"Disp~{fps:.1f} conf:{CONF_THR:.2f} imgsz:{IMG_SIZE}\",\n",
    "                        (10,28), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"ONNX YOLOv8 â€” Video (q quit, -/= conf, [/] size)\", out)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == ord('q'): break\n",
    "        elif k == ord('-'): CONF_THR = max(0.01, CONF_THR - 0.05)\n",
    "        elif k in (ord('='), ord('+')): CONF_THR = min(0.99, CONF_THR + 0.05)\n",
    "        elif k == ord('['): IMG_SIZE = max(320, IMG_SIZE - 64)\n",
    "        elif k == ord(']'): IMG_SIZE = min(1536, IMG_SIZE + 64)\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19926914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, random, subprocess, stat, json\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "import yaml, onnx, onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Small helper: find an executable on PATH\n",
    "def which(cands: List[str]) -> Optional[str]:\n",
    "    for c in cands:\n",
    "        for d in os.environ.get(\"PATH\",\"\").split(os.pathsep):\n",
    "            p = Path(d) / c\n",
    "            if p.exists() and os.access(p, os.X_OK):\n",
    "                return str(p)\n",
    "    return None\n",
    "\n",
    "# Keep all artifacts together\n",
    "WORK_DIR   = Path(\"hailo_build\")\n",
    "EXPORTS    = WORK_DIR / \"exports\"\n",
    "CALIB_DIR  = WORK_DIR / \"calib\"\n",
    "HEF_DIR    = WORK_DIR / \"hef\"\n",
    "REPORT_DIR = WORK_DIR / \"report\"\n",
    "for d in (EXPORTS, CALIB_DIR, HEF_DIR, REPORT_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176c6de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ONNX: hailo_build/exports/best.onnx\n",
      "Input: images\n",
      "Outputs: ['output0']\n",
      "Classes: ['0', '1', '2']\n",
      "num_classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Your trained run + dataset root\n",
    "RUN_DIR       = Path(\"runs/train/soccer_players_clean-exp2\")\n",
    "MODEL_PT      = RUN_DIR / \"weights/best.pt\"\n",
    "DATASET_ROOT  = Path(\"/home/surya/Downloads/soccer players.v1i.yolov8\")\n",
    "IMG_SIZE      = 960         # must match your export\n",
    "CALIB_MAX     = 800         # images for INT8 calibration\n",
    "\n",
    "# Use the latest ONNX produced earlier\n",
    "onnx_list = sorted(EXPORTS.glob(\"*.onnx\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert onnx_list, \"No ONNX in hailo_build/exports â€” run the ONNX export cell first.\"\n",
    "ONNX_PATH = onnx_list[0]\n",
    "print(\"Using ONNX:\", ONNX_PATH)\n",
    "\n",
    "# Parse ONNX I/O names (needed by Hailo)\n",
    "m = onnx.load(str(ONNX_PATH)); onnx.checker.check_model(m)\n",
    "inputs  = list(m.graph.input)\n",
    "outputs = list(m.graph.output)\n",
    "ONNX_INPUT_NAME = inputs[0].name\n",
    "ONNX_OUTPUTS    = [o.name for o in outputs]\n",
    "print(\"Input:\", ONNX_INPUT_NAME)\n",
    "print(\"Outputs:\", ONNX_OUTPUTS)\n",
    "\n",
    "# Find class names\n",
    "def find_data_yaml(root: Path):\n",
    "    if (root / \"data.yaml\").exists(): return root / \"data.yaml\"\n",
    "    for q in root.rglob(\"data.yaml\"):\n",
    "        return q\n",
    "    if (RUN_DIR / \"data.yaml\").exists(): return RUN_DIR / \"data.yaml\"\n",
    "    for q in RUN_DIR.rglob(\"data.yaml\"):\n",
    "        return q\n",
    "    return None\n",
    "\n",
    "DATA_YAML = find_data_yaml(DATASET_ROOT)\n",
    "assert DATA_YAML and DATA_YAML.exists(), f\"data.yaml not found under {DATASET_ROOT} or run dir\"\n",
    "spec = yaml.safe_load(DATA_YAML.read_text())\n",
    "if isinstance(spec.get(\"names\"), dict):\n",
    "    CLASS_NAMES = [spec[\"names\"][i] for i in sorted(map(int, spec[\"names\"].keys()))]\n",
    "else:\n",
    "    CLASS_NAMES = list(spec.get(\"names\", []))\n",
    "NUM_CLASSES = len(CLASS_NAMES) if CLASS_NAMES else spec.get(\"nc\", 80)\n",
    "print(\"Classes:\", CLASS_NAMES)\n",
    "print(\"num_classes:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1191c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calibration images prepared: 163 at /home/surya/c/hailo_build/calib\n"
     ]
    }
   ],
   "source": [
    "# Pick up to CALIB_MAX images from dataset (train/val/test folders)\n",
    "EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
    "cands = [p for p in DATASET_ROOT.rglob(\"*\") if p.suffix.lower() in EXTS]\n",
    "random.seed(42); random.shuffle(cands)\n",
    "pick = cands[:CALIB_MAX] if len(cands) > CALIB_MAX else cands\n",
    "\n",
    "copied = 0\n",
    "for p in pick:\n",
    "    dst = CALIB_DIR / p.name\n",
    "    if not dst.exists():\n",
    "        try:\n",
    "            shutil.copy2(p, dst)\n",
    "            copied += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "print(f\"âœ… Calibration images prepared: {len(list(CALIB_DIR.glob('*')))} at {CALIB_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa239c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Wrote DFC YAML: /home/surya/c/hailo_build/model.yaml\n",
      "network:\n",
      "  name: yolo_v8_custom\n",
      "  inputs:\n",
      "    - name: images\n",
      "      shape: [1, 3, 960, 960]\n",
      "      format: NCHW\n",
      "      normalization:\n",
      "        mean: [0.0, 0.0, 0.0]\n",
      "        std:  [1.0, 1.0, 1.0]\n",
      "        rescale: 1.0\n",
      "  outputs:\n",
      "    - name: output0\n",
      "\n",
      "postprocess:\n",
      "  type: yolo_v8\n",
      "  num_classes: 3\n",
      "  iou_threshold: 0.65\n",
      "  score_threshold: 0.25\n",
      "  input_shape: [960, 960]\n",
      "\n",
      "quantization:\n",
      "  scheme: symmetric\n",
      "  per_channel: true\n",
      "  calibration:\n",
      "    images_dir: hailo_build/calib\n",
      "    max_images: 800\n",
      "\n",
      "compiler:\n",
      "  t ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFC_YAML = WORK_DIR / \"model.yaml\"\n",
    "text = f\"\"\"network:\n",
    "  name: yolo_v8_custom\n",
    "  inputs:\n",
    "    - name: {ONNX_INPUT_NAME}\n",
    "      shape: [1, 3, {IMG_SIZE}, {IMG_SIZE}]\n",
    "      format: NCHW\n",
    "      normalization:\n",
    "        mean: [0.0, 0.0, 0.0]\n",
    "        std:  [1.0, 1.0, 1.0]\n",
    "        rescale: 1.0\n",
    "  outputs:\n",
    "\"\"\"\n",
    "for n in ONNX_OUTPUTS:\n",
    "    text += f\"    - name: {n}\\n\"\n",
    "\n",
    "text += f\"\"\"\n",
    "postprocess:\n",
    "  type: yolo_v8\n",
    "  num_classes: {NUM_CLASSES}\n",
    "  iou_threshold: 0.65\n",
    "  score_threshold: 0.25\n",
    "  input_shape: [{IMG_SIZE}, {IMG_SIZE}]\n",
    "\n",
    "quantization:\n",
    "  scheme: symmetric\n",
    "  per_channel: true\n",
    "  calibration:\n",
    "    images_dir: {CALIB_DIR.as_posix()}\n",
    "    max_images: {CALIB_MAX}\n",
    "\n",
    "compiler:\n",
    "  target_device: hailo8l\n",
    "  output_dir: {HEF_DIR.as_posix()}\n",
    "  output_name: model\n",
    "\"\"\"\n",
    "DFC_YAML.write_text(text)\n",
    "print(\"ðŸ“ Wrote DFC YAML:\", DFC_YAML.resolve())\n",
    "print(DFC_YAML.read_text()[:500], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1347822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: file exists â†’ hailo_build/exports/best.onnx size: 12.45 MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ONNX_PATH = Path(\"hailo_build/exports/best.onnx\")  # or whatever you used\n",
    "assert ONNX_PATH.exists() and ONNX_PATH.stat().st_size > 1_000_000, \"ONNX missing or too small\"\n",
    "print(\"OK: file exists â†’\", ONNX_PATH, \"size:\", round(ONNX_PATH.stat().st_size/1e6, 2), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922addc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: ONNX structure is valid\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m = onnx.load(str(ONNX_PATH))\n",
    "onnx.checker.check_model(m)\n",
    "print(\"OK: ONNX structure is valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588a099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " - images [1, 3, 960, 960]\n",
      "Outputs:\n",
      " - output0 [1, 7, 18900]\n"
     ]
    }
   ],
   "source": [
    "def shape_str(v):\n",
    "    return [d.dim_value if d.HasField(\"dim_value\") else \"?\" for d in v.type.tensor_type.shape.dim]\n",
    "\n",
    "inputs  = list(m.graph.input)\n",
    "outputs = list(m.graph.output)\n",
    "\n",
    "print(\"Inputs:\")\n",
    "for i in inputs:  print(\" -\", i.name, shape_str(i))\n",
    "print(\"Outputs:\")\n",
    "for o in outputs: print(\" -\", o.name, shape_str(o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6308d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: ORT ran. Output shapes: [(1, 7, 18900)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort, numpy as np\n",
    "\n",
    "providers = [\"CPUExecutionProvider\"]\n",
    "if \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
    "    providers = [\"CUDAExecutionProvider\",\"CPUExecutionProvider\"]\n",
    "\n",
    "sess = ort.InferenceSession(str(ONNX_PATH), providers=providers)\n",
    "inp_name = sess.get_inputs()[0].name\n",
    "\n",
    "IMG_SIZE = 960  # match your export size\n",
    "x = np.random.rand(1,3,IMG_SIZE,IMG_SIZE).astype(np.float32)\n",
    "outs = sess.run(None, {inp_name: x})\n",
    "print(\"OK: ORT ran. Output shapes:\", [o.shape for o in outs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58c8897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QuantizeLinear': 0, 'DequantizeLinear': 0, 'QLinearConv': 0, 'Conv': 64}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "ops = Counter(n.op_type for n in m.graph.node)\n",
    "print({k: ops[k] for k in [\"QuantizeLinear\",\"DequantizeLinear\",\"QLinearConv\",\"Conv\"]})\n",
    "# If Quantize*/QLinear* > 0 â†’ quantized ONNX; otherwise float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46a7596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ONNX: hailo_build/exports/best.onnx\n",
      "Input: images\n",
      "Outputs: ['output0']\n",
      "num_classes: 3 | classes: ['0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml, onnx\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "DATASET_ROOT = Path(\"/home/surya/Downloads/soccer players.v1i.yolov8\")\n",
    "RUN_DIR      = Path(\"runs/train/soccer_players_clean-exp2\")\n",
    "IMG_SIZE     = 960          # must match your ONNX export size\n",
    "# =====================\n",
    "\n",
    "WORK_DIR   = Path(\"hailo_build\")\n",
    "EXPORTS    = WORK_DIR / \"exports\"\n",
    "CALIB_DIR  = WORK_DIR / \"calib\"\n",
    "HEF_DIR    = WORK_DIR / \"hef\"\n",
    "for d in (EXPORTS, CALIB_DIR, HEF_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prefer float ONNX (best.onnx) for Hailo; fall back to latest .onnx\n",
    "cands = [p for p in EXPORTS.glob(\"*.onnx\")]\n",
    "assert cands, \"No ONNX found in hailo_build/exports â€” export first.\"\n",
    "prefer = [p for p in cands if \"best.onnx\" in p.name.lower()]\n",
    "ONNX_PATH = prefer[0] if prefer else sorted(cands, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "print(\"Using ONNX:\", ONNX_PATH)\n",
    "\n",
    "# ONNX I/O names (needed for DFC YAML)\n",
    "m = onnx.load(str(ONNX_PATH)); onnx.checker.check_model(m)\n",
    "inputs  = list(m.graph.input);  assert inputs, \"ONNX has no inputs?\"\n",
    "outputs = list(m.graph.output); assert outputs, \"ONNX has no outputs?\"\n",
    "ONNX_INPUT_NAME = inputs[0].name\n",
    "ONNX_OUTPUTS    = [o.name for o in outputs]\n",
    "print(\"Input:\", ONNX_INPUT_NAME)\n",
    "print(\"Outputs:\", ONNX_OUTPUTS)\n",
    "\n",
    "# Get class names from data.yaml (roboflow export places it at the root)\n",
    "def find_data_yaml(root: Path):\n",
    "    if (root / \"data.yaml\").exists(): return root / \"data.yaml\"\n",
    "    for q in root.rglob(\"data.yaml\"): return q\n",
    "    return None\n",
    "\n",
    "DATA_YAML = find_data_yaml(DATASET_ROOT)\n",
    "assert DATA_YAML and DATA_YAML.exists(), f\"data.yaml not found under {DATASET_ROOT}\"\n",
    "spec = yaml.safe_load(DATA_YAML.read_text())\n",
    "names = spec.get(\"names\", [])\n",
    "if isinstance(names, dict):\n",
    "    CLASS_NAMES = [names[i] for i in sorted(map(int, names.keys()))]\n",
    "elif isinstance(names, list):\n",
    "    CLASS_NAMES = names\n",
    "else:\n",
    "    CLASS_NAMES = []\n",
    "NUM_CLASSES = len(CLASS_NAMES) if CLASS_NAMES else spec.get(\"nc\", 80)\n",
    "print(\"num_classes:\", NUM_CLASSES, \"| classes:\", CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fedcad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calibration images prepared: 163 at /home/surya/c/hailo_build/calib\n"
     ]
    }
   ],
   "source": [
    "import shutil, random\n",
    "\n",
    "CALIB_MAX = 800  # 400â€“1000 is typical\n",
    "exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
    "\n",
    "cands = [p for p in DATASET_ROOT.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "random.seed(42); random.shuffle(cands)\n",
    "pick = cands[:CALIB_MAX] if len(cands) > CALIB_MAX else cands\n",
    "\n",
    "copied = 0\n",
    "for p in pick:\n",
    "    dst = CALIB_DIR / p.name\n",
    "    if not dst.exists():\n",
    "        try:\n",
    "            shutil.copy2(p, dst); copied += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f\"âœ… Calibration images prepared: {len(list(CALIB_DIR.glob('*')))} at {CALIB_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Wrote DFC YAML: /home/surya/c/hailo_build/model.yaml\n",
      "--- preview ---\n",
      " network:\n",
      "  name: yolo_v8_custom\n",
      "  inputs:\n",
      "    - name: images\n",
      "      shape: [1, 3, 960, 960]\n",
      "      format: NCHW\n",
      "      normalization:\n",
      "        mean: [0.0, 0.0, 0.0]\n",
      "        std:  [1.0, 1.0, 1.0]\n",
      "        rescale: 1.0\n",
      "  outputs:\n",
      "    - name: output0\n",
      "\n",
      "postprocess:\n",
      "  type: yolo_v8\n",
      "  num_classes: 3\n",
      "  iou_threshold: 0.65\n",
      "  score_threshold: 0.25\n",
      "  input_shape: [960, 960]\n",
      "\n",
      "quantization:\n",
      "  scheme: symmetric\n",
      "  per_channel: true\n",
      "  calibration:\n",
      "    images_dir: hailo_build/calib\n",
      "    max_images: 800\n",
      "\n",
      "compiler:\n",
      "  target_device: hailo8l\n",
      "  output_dir: hailo_build/hef\n",
      "  output_name: model\n",
      " \n",
      "--- end ---\n"
     ]
    }
   ],
   "source": [
    "DFC_YAML = WORK_DIR / \"model.yaml\"\n",
    "\n",
    "text = f\"\"\"network:\n",
    "  name: yolo_v8_custom\n",
    "  inputs:\n",
    "    - name: {ONNX_INPUT_NAME}\n",
    "      shape: [1, 3, {IMG_SIZE}, {IMG_SIZE}]\n",
    "      format: NCHW\n",
    "      normalization:\n",
    "        mean: [0.0, 0.0, 0.0]\n",
    "        std:  [1.0, 1.0, 1.0]\n",
    "        rescale: 1.0\n",
    "  outputs:\n",
    "\"\"\"\n",
    "for n in ONNX_OUTPUTS:\n",
    "    text += f\"    - name: {n}\\n\"\n",
    "\n",
    "text += f\"\"\"\n",
    "postprocess:\n",
    "  type: yolo_v8\n",
    "  num_classes: {NUM_CLASSES}\n",
    "  iou_threshold: 0.65\n",
    "  score_threshold: 0.25\n",
    "  input_shape: [{IMG_SIZE}, {IMG_SIZE}]\n",
    "\n",
    "quantization:\n",
    "  scheme: symmetric\n",
    "  per_channel: true\n",
    "  calibration:\n",
    "    images_dir: {CALIB_DIR.as_posix()}\n",
    "    max_images: {CALIB_MAX}\n",
    "\n",
    "compiler:\n",
    "  target_device: hailo8l\n",
    "  output_dir: {HEF_DIR.as_posix()}\n",
    "  output_name: model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DFC_YAML.write_text(text)\n",
    "print(\"ðŸ“ Wrote DFC YAML:\", DFC_YAML.resolve())\n",
    "print(\"--- preview ---\\n\", DFC_YAML.read_text()[:600], \"\\n--- end ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cde8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_from_shape(shp):\n",
    "    # shp is like [B, C, N] for your model\n",
    "    if len(shp) == 3:   # [1, C, N]\n",
    "        return shp[1]\n",
    "    if len(shp) == 4:   # [1, C, H, W]\n",
    "        return shp[1]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5899541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0', '1', '2']\n",
      "num_classes (NC): 3\n",
      "\n",
      "Inputs:\n",
      " - images [1, 3, 960, 960]\n",
      "Outputs:\n",
      " - output0 [1, 7, 18900]\n",
      "\n",
      "[o=output0] shape=[1, 7, 18900] â†’ BCN | channels on axis 1: C=7 in {4+3,5+3}\n",
      "\n",
      "FINAL VERDICT: MATCH \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml, onnx\n",
    "from onnx import shape_inference\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "ONNX_PATH = Path(\"hailo_build/exports/best.onnx\")   # your best.onnx path\n",
    "DATA_YAML = Path(\"/home/surya/Downloads/soccer players.v1i.yolov8/data.yaml\")\n",
    "IMG_SIZE  = 960\n",
    "# ====================\n",
    "\n",
    "def dims_list(v):\n",
    "    out = []\n",
    "    for d in v.type.tensor_type.shape.dim:\n",
    "        if d.HasField(\"dim_value\"): out.append(int(d.dim_value))\n",
    "        elif d.HasField(\"dim_param\") and d.dim_param: out.append(d.dim_param)\n",
    "        else: out.append(\"?\")\n",
    "    return out\n",
    "\n",
    "# Load classes\n",
    "spec = yaml.safe_load(DATA_YAML.read_text())\n",
    "names = spec.get(\"names\", [])\n",
    "if isinstance(names, dict):\n",
    "    CLASS_NAMES = [names[i] for i in sorted(map(int, names.keys()))]\n",
    "elif isinstance(names, list):\n",
    "    CLASS_NAMES = names\n",
    "else:\n",
    "    CLASS_NAMES = []\n",
    "NC = len(CLASS_NAMES) if CLASS_NAMES else int(spec.get(\"nc\", 80))\n",
    "print(\"Classes:\", CLASS_NAMES)\n",
    "print(\"num_classes (NC):\", NC)\n",
    "\n",
    "# Load ONNX & infer shapes\n",
    "m = onnx.load(str(ONNX_PATH))\n",
    "onnx.checker.check_model(m)\n",
    "try:\n",
    "    mi = shape_inference.infer_shapes(m)\n",
    "except Exception:\n",
    "    mi = m\n",
    "\n",
    "inputs  = list(mi.graph.input)\n",
    "outputs = list(mi.graph.output)\n",
    "print(\"\\nInputs:\")\n",
    "for i in inputs:  print(\" -\", i.name, dims_list(i))\n",
    "print(\"Outputs:\")\n",
    "for o in outputs: print(\" -\", o.name, dims_list(o))\n",
    "\n",
    "# Robust head checker: accepts [B,C,N] or [B,N,C] or [B,C,H,W]\n",
    "def check_head_shape(shape, nc):\n",
    "    # returns (ok:bool, layout:str, C:int|None, N:int|None, msg:str)\n",
    "    if len(shape) == 4:\n",
    "        b,c,h,w = shape\n",
    "        if isinstance(c, int) and c in (4+nc, 5+nc):\n",
    "            return True, \"BCHW\", c, (h,w), f\"C={c} in {{4+{nc},5+{nc}}}\"\n",
    "        return False, \"BCHW\", c if isinstance(c,int) else None, (h,w), f\"C={c} not in {{4+{nc},5+{nc}}}\"\n",
    "\n",
    "    if len(shape) == 3:\n",
    "        b,d1,d2 = shape\n",
    "        cand = []\n",
    "        for ax, dim in enumerate([d1,d2], start=1):  # axes 1 or 2 can be C\n",
    "            if isinstance(dim,int) and dim in (4+nc, 5+nc):\n",
    "                layout = \"BCN\" if ax==1 else \"BNC\"\n",
    "                C = dim\n",
    "                N = d2 if ax==1 else d1\n",
    "                return True, layout, C, N, f\"channels on axis {ax}: C={C} in {{4+{nc},5+{nc}}}\"\n",
    "        return False, \"B?N\", None, None, f\"no axis equals 4+{nc} or 5+{nc} (got {d1},{d2})\"\n",
    "\n",
    "    return False, \"other\", None, None, f\"unsupported rank: {len(shape)}\"\n",
    "\n",
    "# Static checks\n",
    "any_ok = False\n",
    "for o in outputs:\n",
    "    shp = dims_list(o)\n",
    "    ok, layout, C, N, msg = check_head_shape(shp, NC)\n",
    "    any_ok |= ok\n",
    "    print(f\"\\n[o={o.name}] shape={shp} â†’ {layout} | {msg}\")\n",
    "\n",
    "# If shape ranks/values were unknown, confirm with ORT\n",
    "if not any_ok or any(\"?\" in map(str,dims_list(o)) for o in outputs):\n",
    "    sess = ort.InferenceSession(str(ONNX_PATH), providers=[\"CPUExecutionProvider\"])\n",
    "    inp  = sess.get_inputs()[0].name\n",
    "    x = np.random.rand(1,3,IMG_SIZE,IMG_SIZE).astype(np.float32)\n",
    "    outs = sess.run(None, {inp: x})\n",
    "    print(\"\\nRuntime output shapes:\", [list(a.shape) for a in outs])\n",
    "    # Re-check using runtime shapes\n",
    "    any_ok_rt = False\n",
    "    for a in outs:\n",
    "        ok, layout, C, N, msg = check_head_shape(list(a.shape), NC)\n",
    "        any_ok_rt |= ok\n",
    "        print(\" - runtime:\", list(a.shape), \"â†’\", layout, \"|\", msg)\n",
    "    any_ok = any_ok or any_ok_rt\n",
    "\n",
    "print(\"\\nFINAL VERDICT:\", \"MATCH \" if any_ok else \"MISMATCH \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887962b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surya/c/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:732: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Print model summary\n",
    "model.info()\n",
    "\n",
    "# Visualize model (requires graphviz installed)\n",
    "model.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d45822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
